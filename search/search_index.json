{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OntoSage 2.0 Documentation Portal","text":"<p>Welcome to the official documentation site for OntoSage 2.0 \u2014 an Agentic AI platform for Intelligent Buildings.</p> <p>OntoSage 2.0 is designed to bridge the gap between complex building data (Ontologies, Time-Series, Metadata) and human users (Facility Managers, Researchers, Students) through a natural language interface.</p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Multi-Agent Orchestration: Specialized agents for Dialogue, SPARQL (Ontology), SQL (Time-Series), Analytics, and Visualization.</li> <li>Hybrid RAG: Combines Vector Search (Qdrant) with Semantic Graph Queries (GraphDB).</li> <li>Secure Analytics: Sandboxed Python code execution for on-the-fly data analysis.</li> <li>High Performance: Redis-based caching for conversation state and semantic queries.</li> <li>Robust Security: Hardened SQL execution and deterministic analytics templates.</li> <li>Interactive UI: React-based frontend with 3D Digital Twin and Plotly charts.</li> </ul>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":""},{"location":"#core-concepts","title":"Core Concepts","text":"<ul> <li>Architecture: High-level system design, component interaction, and agent roles.</li> <li>Workflow Deep Dive: Detailed \"under-the-hood\" explanation of how requests are processed, from frontend to agent execution.</li> <li>Services: Catalog of all microservices (Orchestrator, RAG, Code Executor, etc.) with port mappings and dependencies.</li> </ul>"},{"location":"#guides","title":"Guides","text":"<ul> <li>User Guide: How to use the chat interface, voice commands, and interpret visualizations.</li> <li>Developer Guide: Setup, testing, debugging, and contributing to the codebase.</li> <li>Deployment: Instructions for Docker Compose deployment (Local &amp; Cloud).</li> <li>Building Onboarding: How to import your own building topology (Brick Schema/RealEstateCore .ttl files).</li> <li>Configuration: Environment variables and system settings.</li> <li>Security: Authentication, authorization, and sandbox security.</li> <li>Runbook: Operational procedures for maintenance and troubleshooting.</li> </ul>"},{"location":"#system-diagram","title":"\ud83c\udfd7\ufe0f System Diagram","text":"<pre><code>graph TD\n  User((User)) --&gt;|Chat/Voice| FE[Frontend UI]\n  FE --&gt;|REST/WS| ORCH[Orchestrator]\n\n  subgraph \"Agentic Core\"\n    ORCH --&gt;|Routes| Dialogue[Dialogue Agent]\n    ORCH --&gt;|Routes| SPARQL[SPARQL Agent]\n    ORCH --&gt;|Routes| SQL[SQL Agent]\n    ORCH --&gt;|Routes| Analytics[Analytics Agent]\n    ORCH --&gt;|Routes| Vis[Visualization Agent]\n  end\n\n  subgraph \"Data &amp; Knowledge\"\n    SPARQL --&gt;|Query| GDB[(GraphDB)]\n    SQL --&gt;|Query| MYSQL[(MySQL)]\n    Dialogue --&gt;|Retrieve| RAG[RAG Service]\n    RAG --&gt;|Search| QDR[(Qdrant)]\n    ORCH --&gt;|Cache| REDIS[(Redis)]\n  end\n\n  subgraph \"Execution\"\n    Analytics --&gt;|Run Code| EXEC[Code Executor]\n    ORCH --&gt;|Transcribe| WHIS[Whisper STT]\n  end</code></pre>"},{"location":"#quick-start","title":"\ud83c\udfc1 Quick Start","text":"<ol> <li>Deploy: Follow the Deployment Guide to start the stack.</li> <li>Onboard: Import your building data using the Onboarding Guide.</li> <li>Chat: Open <code>http://localhost:3000</code> and ask \"What is the temperature in Room 101?\".</li> </ol>"},{"location":"ARCHITECTURE/","title":"System Architecture","text":"<p>OntoSage 2.0 employs a modular, microservices-based architecture designed for scalability, flexibility, and robust agentic behavior. The system is orchestrated using LangGraph to manage state and interactions between specialized AI agents.</p>"},{"location":"ARCHITECTURE/#high-level-overview","title":"High-Level Overview","text":"<p>The system follows a hub-and-spoke model where the Orchestrator serves as the central hub, coordinating requests from the Frontend and delegating tasks to specialized services and agents.</p> <pre><code>graph TD\n    User[User] --&gt;|HTTP/WS| Frontend[\"Frontend (React)\"]\n    Frontend --&gt;|REST API| Orchestrator[\"Orchestrator (FastAPI/LangGraph)\"]\n\n    subgraph \"Agentic Core\"\n        Orchestrator --&gt;|Delegates| DialogueAgent[\"Dialogue Agent\"]\n        Orchestrator --&gt;|Delegates| SPARQLAgent[\"SPARQL Agent\"]\n        Orchestrator --&gt;|Delegates| SQLAgent[\"SQL Agent\"]\n        Orchestrator --&gt;|Delegates| AnalyticsAgent[\"Analytics Agent\"]\n        Orchestrator --&gt;|Delegates| VisAgent[\"Visualization Agent\"]\n    end\n\n    subgraph \"Support Services\"\n        SPARQLAgent --&gt;|Query| RAGService[\"RAG Service\"]\n        RAGService --&gt;|Vector Search| Qdrant[\"Qdrant Vector DB\"]\n        RAGService --&gt;|Semantic Query| Fuseki[\"Fuseki Knowledge Graph\"]\n\n        SQLAgent --&gt;|Query| SQLDB[(\"MySQL/PostgreSQL\")]\n\n        AnalyticsAgent --&gt;|Execute| CodeExecutor[\"Code Executor (Sandbox)\"]\n\n        Orchestrator --&gt;|Audio| WhisperSTT[\"Whisper STT\"]\n\n        Orchestrator --&gt;|State| Redis[\"Redis Cache\"]\n    end\n\n    subgraph \"Model Providers\"\n        Orchestrator -.-&gt;|Inference| Ollama[\"Ollama (Local LLM)\"]\n        Orchestrator -.-&gt;|Inference| OpenAI[\"OpenAI (Cloud LLM)\"]\n    end</code></pre>"},{"location":"ARCHITECTURE/#core-components","title":"Core Components","text":""},{"location":"ARCHITECTURE/#1-orchestrator-the-brain","title":"1. Orchestrator (The Brain)","text":"<ul> <li>Technology: Python, FastAPI, LangGraph.</li> <li>Role: Manages the conversation flow, maintains context, and routes user intents to the appropriate agent.</li> <li>Key Features:</li> <li>Redis Caching: Implements semantic caching for SPARQL and SQL queries (1-hour TTL) and persists conversation state.</li> <li>API Standardization: All endpoints return a uniform JSON structure (<code>{ success, data, error }</code>).</li> <li>Agents:</li> <li>Dialogue Agent: Handles general conversation and intent classification.</li> <li>SPARQL Agent: Translates natural language to SPARQL queries for ontology interaction.</li> <li>SQL Agent: Generates SQL queries for time-series data retrieval. Hardened to allow only <code>SELECT</code> statements and prevent injection.</li> <li>Analytics Agent: Generates Python code to analyze data. Uses Deterministic Templates for common tasks to ensure reliability.</li> <li>Visualization Agent: Creates Plotly charts for data visualization.</li> </ul>"},{"location":"ARCHITECTURE/#2-rag-service-knowledge-retrieval","title":"2. RAG Service (Knowledge Retrieval)","text":"<ul> <li>Technology: Python, Qdrant, Sentence Transformers.</li> <li>Role: Provides semantic search capabilities over the building ontology and documentation. It retrieves relevant context to augment LLM responses.</li> </ul>"},{"location":"ARCHITECTURE/#3-code-executor-safe-execution","title":"3. Code Executor (Safe Execution)","text":"<ul> <li>Technology: Python, Docker/Sandbox.</li> <li>Role: Executes Python code generated by the Analytics Agent in an isolated environment to prevent security risks. It returns the output (text or image) to the Orchestrator.</li> </ul>"},{"location":"ARCHITECTURE/#4-whisper-stt-voice-interface","title":"4. Whisper STT (Voice Interface)","text":"<ul> <li>Technology: OpenAI Whisper (Local or Cloud).</li> <li>Role: Transcribes voice input from the frontend into text for the Orchestrator to process.</li> </ul>"},{"location":"ARCHITECTURE/#5-frontend-user-interface","title":"5. Frontend (User Interface)","text":"<ul> <li>Technology: React 19, TypeScript, Tailwind CSS.</li> <li>Role: Provides a chat interface, 3D building visualization, and interactive charts. It communicates with the backend via REST APIs and WebSockets.</li> </ul>"},{"location":"ARCHITECTURE/#data-layer","title":"Data Layer","text":"<ul> <li>Redis: Stores conversation history and agent state for low-latency access.</li> <li>Qdrant: Vector database for storing embeddings of the ontology and documents.</li> <li>Apache Jena Fuseki: Stores the RDF Knowledge Graph (Ontology).</li> <li>MySQL / PostgreSQL: Stores structured building data (sensors, time-series telemetry).</li> </ul>"},{"location":"ARCHITECTURE/#deployment-infrastructure","title":"Deployment &amp; Infrastructure","text":"<ul> <li>Docker Compose: Orchestrates the containerized services.</li> <li>Monitoring: Prometheus collects metrics, and Grafana provides dashboards for system health and performance.</li> <li>Model Serving: </li> <li>Local: Ollama serves open-source models (e.g., Llama 3, Mistral) locally.</li> <li>Cloud: Direct integration with OpenAI API for GPT-4 models.</li> </ul>"},{"location":"BUILDING_ONBOARDING/","title":"Building Onboarding Guide (TTL)","text":"<p>This guide explains how to load your own building ontology (Turtle <code>.ttl</code>) and start chatting with the system.</p>"},{"location":"BUILDING_ONBOARDING/#1-prepare-your-ontology","title":"1) Prepare Your Ontology","text":"<p>Your ontology should include: - Sites, Buildings, Floors, Rooms (locations) - Equipment (HVAC, Lighting, Meters) - Points/Sensors and relationships (<code>isLocationOf</code>, <code>hasPoint</code>, <code>feeds</code>) - Prefer Brick schema alignment where possible</p>"},{"location":"BUILDING_ONBOARDING/#2-place-files","title":"2) Place Files","text":"<pre><code>mkdir -p data/my_building/dataset\ncp /path/to/your/building.ttl data/my_building/dataset/\n</code></pre>"},{"location":"BUILDING_ONBOARDING/#3-configure-graphdb-import","title":"3) Configure GraphDB Import","text":"<p>Edit <code>docker-compose.agentic.yml</code> and update GraphDB volume mapping: <pre><code>  graphdb:\n    volumes:\n      - ./volumes/graphdb:/opt/graphdb/home\n      - ./data/my_building/dataset:/opt/graphdb/import:ro\n</code></pre></p> <p>Restart GraphDB: <pre><code>docker-compose -f docker-compose.agentic.yml restart graphdb\n</code></pre></p> <p>If the repository is empty, GraphDB will import from <code>/opt/graphdb/import</code> on first run. Otherwise, use the GraphDB Workbench to import your TTL.</p>"},{"location":"BUILDING_ONBOARDING/#4-optional-initialize-rag-indexes","title":"4) (Optional) Initialize RAG Indexes","text":"<p>If you want RAG over descriptions and metadata: <pre><code>docker-compose -f docker-compose.agentic.yml run --rm rag-service python -m scripts.init_qdrant\n# Ingest ontology text\ndocker-compose -f docker-compose.agentic.yml run --rm rag-service python -m scripts.ingest_ontology --source /staging\n</code></pre></p>"},{"location":"BUILDING_ONBOARDING/#5-map-sensors-to-time-series-sql","title":"5) Map Sensors to Time-Series (SQL)","text":"<p>Ensure your sensor identifiers match those used in the MySQL time-series database. Provide a mapping file or follow the existing naming convention (UUIDs).</p>"},{"location":"BUILDING_ONBOARDING/#6-validate-with-sample-queries","title":"6) Validate with Sample Queries","text":"<ul> <li>Structural: <pre><code>SELECT ?sensor WHERE { ?sensor a brick:Temperature_Sensor }\n</code></pre></li> <li>Time-Series: \"Average temperature for Room 101 last week\"</li> <li>Analytics: \"Plot weekly average humidity for Floor 2 over 90 days\"</li> </ul>"},{"location":"BUILDING_ONBOARDING/#7-troubleshooting","title":"7) Troubleshooting","text":"<ul> <li>Graph not updated: clear GraphDB repository and re-import</li> <li>Missing sensors: verify class IRIs and prefixes</li> <li>SQL agent returns empty: verify sensor UUID mapping and date range</li> </ul>"},{"location":"CONFIGURATION/","title":"Configuration Guide","text":"<p>Centralized configuration is handled via <code>.env</code> and service-level environment variables.</p>"},{"location":"CONFIGURATION/#model-provider-selection","title":"Model Provider Selection","text":"<ul> <li><code>MODEL_PROVIDER</code>: <code>local</code> | <code>cloud</code> (default <code>local</code>)</li> <li>Local (Ollama):</li> <li><code>OLLAMA_BASE_URL</code>: e.g., <code>http://ollama-deepseek-r1:11434</code></li> <li><code>OLLAMA_MODEL</code>: e.g., <code>deepseek-r1:32b</code></li> <li>Performance: <code>OLLAMA_NUM_CTX</code>, <code>OLLAMA_KEEP_ALIVE</code>, <code>OLLAMA_GPU_LAYERS</code></li> <li>Cloud (OpenAI):</li> <li><code>OPENAI_API_KEY</code></li> <li><code>OPENAI_MODEL</code> (e.g., <code>gpt-4o-mini</code>)</li> </ul>"},{"location":"CONFIGURATION/#orchestrator","title":"Orchestrator","text":"<ul> <li><code>REDIS_HOST</code>, <code>REDIS_PORT</code></li> <li><code>RAG_SERVICE_HOST</code>, <code>RAG_SERVICE_PORT</code></li> <li><code>CODE_EXECUTOR_HOST</code>, <code>CODE_EXECUTOR_PORT</code></li> <li><code>WHISPER_STT_HOST</code>, <code>WHISPER_STT_PORT</code></li> </ul>"},{"location":"CONFIGURATION/#graphdb-setup","title":"GraphDB Setup","text":"<p>For detailed instructions on setting up the GraphDB Similarity Index, please refer to the GraphDB Setup Guide.</p> <ul> <li><code>GRAPHDB_HOST</code>, <code>GRAPHDB_PORT</code></li> <li><code>MYSQL_HOST</code>, <code>MYSQL_PORT</code></li> <li><code>LLM_TEMPERATURE</code></li> <li><code>USE_SEMANTIC_ONTOLOGY=true|false</code></li> <li><code>ONTOLOGY_QUERY_MODE=semantic|sparql</code></li> </ul>"},{"location":"CONFIGURATION/#databases","title":"Databases","text":"<ul> <li>MySQL:</li> <li><code>MYSQL_ROOT_PASSWORD</code>, <code>MYSQL_DATABASE</code>, <code>MYSQL_USER</code>, <code>MYSQL_PASSWORD</code></li> <li>Postgres (user data):</li> <li><code>POSTGRES_USER_DB</code>, <code>POSTGRES_USER_USER</code>, <code>POSTGRES_USER_PASSWORD</code></li> </ul>"},{"location":"CONFIGURATION/#rag","title":"RAG","text":"<ul> <li><code>GRAPHDB_REPOSITORY</code> (default <code>bldg</code>)</li> <li><code>GRAPHDB_SIMILARITY_INDEX</code> (default <code>bldg_index</code>)</li> </ul>"},{"location":"CONFIGURATION/#whisper-stt","title":"Whisper STT","text":"<ul> <li><code>WHISPER_MODEL</code>: <code>tiny-int8</code> | <code>base</code> | <code>small-int8</code> | <code>medium</code></li> <li><code>WHISPER_BEAM</code>: <code>1</code> for speed, higher for accuracy</li> <li><code>WHISPER_LANG</code>: <code>en</code> (set explicitly for best results)</li> </ul>"},{"location":"CONFIGURATION/#ports-networks","title":"Ports &amp; Networks","text":"<p>Reference: <code>docker-compose.agentic.yml</code> defines ports and two networks: - <code>ontobot-agentic</code> (internal) - <code>ontobot-network</code> (external integration)</p>"},{"location":"CONFIGURATION/#volumes","title":"Volumes","text":"<ul> <li><code>./volumes/*</code> for persistent state (ollama, qdrant, redis, graphdb, mongo, artifacts)</li> <li><code>mysql-data</code> external volume for MySQL</li> </ul>"},{"location":"DEPLOYMENT/","title":"Deployment Guide","text":"<p>This guide covers the deployment of OntoSage 2.0 using Docker Compose.</p>"},{"location":"DEPLOYMENT/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop (Windows/Mac) or Docker Engine (Linux)</li> <li>NVIDIA GPU Drivers (Optional, for local LLM acceleration)</li> <li>PowerShell (Windows) or Bash (Linux/Mac)</li> </ul>"},{"location":"DEPLOYMENT/#quick-start","title":"Quick Start","text":"<p>The easiest way to deploy is using the automated startup script.</p>"},{"location":"DEPLOYMENT/#1-configure-environment","title":"1. Configure Environment","text":"<p>Copy the appropriate environment template to <code>.env</code>:</p> <p>For Local Models (Free, requires decent hardware): <pre><code>cp .env.local .env\n</code></pre></p> <p>For Cloud Models (OpenAI, paid): <pre><code>cp .env.cloud .env\n</code></pre> Edit <code>.env</code> to add your <code>OPENAI_API_KEY</code> if using cloud models.</p>"},{"location":"DEPLOYMENT/#2-run-startup-script","title":"2. Run Startup Script","text":"<p>Windows (PowerShell): <pre><code>./startup.ps1 -Provider local  # or -Provider cloud\n</code></pre></p> <p>Linux/Mac (Bash): <pre><code>./scripts/check-health.sh\ndocker-compose up -d\n</code></pre></p>"},{"location":"DEPLOYMENT/#manual-deployment","title":"Manual Deployment","text":"<p>If you prefer to run Docker Compose commands manually:</p> <pre><code># Build and start core services\ndocker-compose up -d --build\n\n# If using Agentic features (Ollama/Open WebUI)\ndocker-compose -f docker-compose.agentic.yml up -d\n</code></pre>"},{"location":"DEPLOYMENT/#service-endpoints","title":"Service Endpoints","text":"Service URL Description Frontend <code>http://localhost:3000</code> Main User Interface Orchestrator API <code>http://localhost:8000</code> Backend API &amp; Swagger Docs RAG Service <code>http://localhost:8001</code> Retrieval Service Code Executor <code>http://localhost:8002</code> Sandbox Service Whisper STT <code>http://localhost:8003</code> Speech-to-Text Service Ollama <code>http://localhost:11434</code> Local LLM Server Grafana <code>http://localhost:3001</code> Monitoring Dashboard (if enabled)"},{"location":"DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Ollama Model Download: The first run may take a while as it downloads the LLM (several GBs). Check logs with <code>docker logs -f ollama</code>.</li> <li>Port Conflicts: Ensure ports 3000, 8000, 5432, 3306 are free. You can change ports in <code>.env</code>.</li> <li>GPU Issues: If using local models, ensure Docker has access to your GPU.</li> </ul>"},{"location":"DEVELOPER_GUIDE/","title":"Developer Guide","text":""},{"location":"DEVELOPER_GUIDE/#local-development","title":"Local Development","text":"<ul> <li>Python 3.11; create a venv and install per-service <code>requirements.txt</code>.</li> <li>Orchestrator and shared modules are bind-mounted for hot reload in Docker.</li> </ul>"},{"location":"DEVELOPER_GUIDE/#codebase-layout","title":"Codebase Layout","text":"<ul> <li><code>orchestrator/</code>: FastAPI app, LangGraph workflow, agents, managers<ul> <li><code>agents/</code>: Individual agent logic (Dialogue, SPARQL, SQL, Analytics)</li> <li><code>services/</code>: Helper services (ContextManager)</li> <li><code>redis_manager.py</code>: Caching and state persistence</li> <li><code>postgres_manager.py</code>: Long-term storage</li> </ul> </li> <li><code>rag-service/</code>: GraphDB/Qdrant RAG components</li> <li><code>code-executor/</code>: Sandbox runtime and API</li> <li><code>frontend/</code>: React 19 + TypeScript UI</li> <li><code>shared/</code>: Settings (<code>shared/config.py</code>) and common utilities (<code>shared/models.py</code>)</li> </ul>"},{"location":"DEVELOPER_GUIDE/#adding-an-agent","title":"Adding an Agent","text":"<ol> <li>Create file in <code>orchestrator/agents/</code> (e.g., <code>my_agent.py</code>).</li> <li>Define tool functions and state IO.</li> <li>Register in <code>orchestrator/workflow.py</code> and routing logic.</li> <li>Expose any needed config via <code>shared/config.py</code>.</li> </ol>"},{"location":"DEVELOPER_GUIDE/#api-standardization","title":"API Standardization","text":"<p>All API endpoints must return the <code>APIResponse</code> model defined in <code>shared/models.py</code>. <pre><code>return APIResponse(\n    success=True,\n    data={\"key\": \"value\"},\n    error=None\n)\n</code></pre> Refer to <code>orchestrator/main.py</code> for examples.</p>"},{"location":"DEVELOPER_GUIDE/#caching-strategy","title":"Caching Strategy","text":"<ul> <li>Conversation State: Automatically saved to Redis after every turn.</li> <li>Semantic Caching: SPARQL and SQL agents check <code>redis_manager</code> for cached results before executing queries. Use <code>generate_hash(query)</code> for keys.</li> </ul>"},{"location":"DEVELOPER_GUIDE/#testing","title":"Testing","text":"<p><pre><code>pytest -q\n</code></pre> End-to-end smoke tests can be run with: <pre><code>python smoke_test.py\n</code></pre></p>"},{"location":"DEVELOPER_GUIDE/#debugging","title":"Debugging","text":"<ul> <li>Tail orchestrator logs: <code>docker-compose -f docker-compose.agentic.yml logs -f orchestrator</code></li> <li>Attach VSCode debugger to Python inside the container if needed.</li> </ul>"},{"location":"DEVELOPER_GUIDE/#style","title":"Style","text":"<ul> <li>Follow PEP8; type hints encouraged</li> <li>Keep agents small and single-responsibility</li> </ul>"},{"location":"DEVELOPER_GUIDE/#api","title":"API","text":"<ul> <li>FastAPI docs at <code>http://localhost:8000/docs</code></li> <li>OpenAI-compatible endpoints under <code>/v1</code> if enabled</li> </ul>"},{"location":"GRAPHDB_SETUP/","title":"GraphDB Similarity Index Setup Guide","text":""},{"location":"GRAPHDB_SETUP/#manual-index-creation-via-graphdb-workbench","title":"Manual Index Creation via GraphDB Workbench","text":"<p>Since GraphDB's Similarity Plugin is designed to be configured via the Workbench UI, follow these steps to create the index for your <code>bldg</code> repository:</p>"},{"location":"GRAPHDB_SETUP/#step-1-access-graphdb-workbench","title":"Step 1: Access GraphDB Workbench","text":"<ol> <li>Open your browser and navigate to: <code>http://localhost:7200</code></li> <li>Select the <code>bldg</code> repository from the repository dropdown (top right)</li> </ol>"},{"location":"GRAPHDB_SETUP/#step-2-create-text-similarity-index","title":"Step 2: Create Text Similarity Index","text":"<ol> <li>Navigate to Explore \u2192 Similarity \u2192 Create similarity index</li> <li>Click Create text similarity index</li> </ol>"},{"location":"GRAPHDB_SETUP/#step-3-configure-the-index","title":"Step 3: Configure the Index","text":""},{"location":"GRAPHDB_SETUP/#index-name","title":"Index Name","text":"<pre><code>bldg_index\n</code></pre>"},{"location":"GRAPHDB_SETUP/#data-query-what-to-index","title":"Data Query (What to Index)","text":"<p>Paste this SPARQL query: <pre><code>PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX brick: &lt;https://brickschema.org/schema/Brick#&gt;\n\nSELECT ?documentID ?documentText {\n    ?documentID rdf:type ?type .\n    FILTER(ISIRI(?documentID))\n\n    # Get Label (optional)\n    OPTIONAL { ?documentID rdfs:label ?label }\n\n    # Get Type Name (strip namespace)\n    BIND(REPLACE(STR(?type), \"^.*[#/]([^#/]+)$\", \"$1\") as ?typeName)\n\n    # Get Entity Name (strip namespace)\n    BIND(REPLACE(STR(?documentID), \"^.*[#/]([^#/]+)$\", \"$1\") as ?entityName)\n\n    # Combine into a rich document text\n    BIND(CONCAT(\n        COALESCE(?label, \"\"), \" \", \n        COALESCE(?typeName, \"\"), \" \", \n        COALESCE(?entityName, \"\")\n    ) as ?documentText)\n}\n</code></pre></p>"},{"location":"GRAPHDB_SETUP/#advanced-settings-click-more-options","title":"Advanced Settings (Click \"more options\")","text":"<ol> <li>Analyzer Class: <code>org.apache.lucene.analysis.en.EnglishAnalyzer</code></li> <li>Stop Words: <code>a,an,the,and,or,of,to,in</code></li> <li>Semantic Vectors create index parameters:    <pre><code>-termweight idf -dimension 300 -minfrequency 2\n</code></pre></li> </ol>"},{"location":"GRAPHDB_SETUP/#step-4-create-the-index","title":"Step 4: Create the Index","text":"<ol> <li>Click the Create button</li> <li>Wait for the index to build (may take several minutes for 93,237 triples)</li> <li>You'll see a spinning icon next to <code>bldg_index</code> while it builds</li> </ol>"},{"location":"GRAPHDB_SETUP/#step-5-verify-the-index","title":"Step 5: Verify the Index","text":"<p>Once complete, <code>bldg_index</code> will appear in the \"Existing Indexes\" list with icons to: - View SPARQL query - Edit query - Rebuild index - Delete index</p>"},{"location":"GRAPHDB_SETUP/#alternative-sparql-based-rebuild-if-index-already-exists","title":"Alternative: SPARQL-based Rebuild (if index already exists)","text":"<p>If you manually create the index via UI first, you can rebuild it via SPARQL:</p> <pre><code>PREFIX similarity-index: &lt;http://www.ontotext.com/graphdb/similarity/instance/&gt;\nPREFIX similarity: &lt;http://www.ontotext.com/graphdb/similarity/&gt;\n\nINSERT DATA {\n    similarity-index:bldg_index similarity:rebuildIndex \"\" .\n}\n</code></pre>"},{"location":"GRAPHDB_SETUP/#testing-the-index","title":"Testing the Index","text":"<p>Once the index is built, test it via SPARQL:</p> <pre><code>PREFIX : &lt;http://www.ontotext.com/graphdb/similarity/&gt;\nPREFIX inst: &lt;http://www.ontotext.com/graphdb/similarity/instance/&gt;\n\nSELECT ?entity ?score WHERE {\n    ?search a inst:bldg_index ;\n           :searchTerm \"temperature sensor\" ;\n           :documentResult ?result .\n    ?result :value ?entity ;\n           :score ?score .\n}\nORDER BY DESC(?score)\nLIMIT 10\n</code></pre>"},{"location":"PROJECT_STRUCTURE/","title":"Project Structure Guide","text":"<p>This document provides a detailed overview of the OntoSage 2.0 project structure.</p>"},{"location":"PROJECT_STRUCTURE/#root-directory","title":"Root Directory","text":"File/Folder Description <code>docker-compose.yml</code> Main Docker Compose configuration for the core services. <code>docker-compose.*.yml</code> Specialized Docker Compose files for different environments (Agentic, Buildings, DBs). <code>startup.ps1</code> PowerShell script for automated deployment and startup. <code>README.md</code> Quick start guide and project overview. <code>Abacws/</code> Contains the legacy/integration components for the Abacws system (API, Visualiser). <code>Assets/</code> Stores static assets, datasets, and data processing notebooks. <code>bldg1/</code>, <code>bldg2/</code> Configuration and data specific to different building deployments. <code>code-executor/</code> Service for securely executing generated Python code (Analytics Agent). <code>data/</code> Shared data directory for services. <code>deploy/</code> Deployment scripts and configuration files. <code>docs/</code> Project documentation (Architecture, Guides, References). <code>frontend/</code> React-based user interface application. <code>monitoring/</code> Configuration for Prometheus and Grafana monitoring. <code>ollama-health/</code> Health check sidecar service for Ollama. <code>orchestrator/</code> The core FastAPI application managing the LangGraph agents. <code>rag-service/</code> Service handling Retrieval-Augmented Generation and Vector DB interactions. <code>scripts/</code> Utility scripts for health checks, setup, and maintenance. <code>shared/</code> Shared code libraries or utilities used across multiple services. <code>tests/</code> Automated test suite. <code>volumes/</code> Docker volumes for persistent data storage (Database data, Ollama models). <code>whisper-stt/</code> Service for Speech-to-Text processing using OpenAI Whisper."},{"location":"PROJECT_STRUCTURE/#service-directories","title":"Service Directories","text":""},{"location":"PROJECT_STRUCTURE/#orchestrator","title":"<code>orchestrator/</code>","text":"<p>The brain of the system. - <code>app/</code>: Main application code.   - <code>agents/</code>: Definitions for Dialogue, SPARQL, SQL, Analytics, and Visualization agents.   - <code>core/</code>: Core logic, configuration, and shared utilities.   - <code>models/</code>: Pydantic models and database schemas.   - <code>api/</code>: API route definitions.</p>"},{"location":"PROJECT_STRUCTURE/#rag-service","title":"<code>rag-service/</code>","text":"<p>Handles knowledge retrieval. - <code>app/</code>: Application code.   - <code>embeddings/</code>: Logic for generating text embeddings.   - <code>vector_store/</code>: Interface for Qdrant vector database.   - <code>retriever/</code>: Logic for semantic search and context retrieval.</p>"},{"location":"PROJECT_STRUCTURE/#frontend","title":"<code>frontend/</code>","text":"<p>The user interface. - <code>src/</code>: Source code.   - <code>components/</code>: Reusable UI components (Chat interface, 3D viewer).   - <code>hooks/</code>: Custom React hooks.   - <code>services/</code>: API client services.   - <code>types/</code>: TypeScript type definitions.</p>"},{"location":"PROJECT_STRUCTURE/#code-executor","title":"<code>code-executor/</code>","text":"<p>Sandboxed execution environment. - <code>app/</code>: Application code.   - <code>sandbox/</code>: Logic for isolating and running Python code safely.</p>"},{"location":"PROJECT_STRUCTURE/#whisper-stt","title":"<code>whisper-stt/</code>","text":"<p>Voice processing. - <code>app/</code>: Application code.   - <code>audio/</code>: Audio processing utilities.   - <code>model/</code>: Whisper model management.</p>"},{"location":"PROJECT_STRUCTURE/#data-configuration","title":"Data &amp; Configuration","text":""},{"location":"PROJECT_STRUCTURE/#assets","title":"<code>Assets/</code>","text":"<ul> <li><code>*.json</code>: Raw and processed datasets for buildings.</li> <li><code>*.ipynb</code>: Jupyter notebooks used for data cleaning and preparation.</li> </ul>"},{"location":"PROJECT_STRUCTURE/#volumes","title":"<code>volumes/</code>","text":"<p>Note: This directory is often excluded from version control but is crucial for local state. - <code>qdrant_data/</code>: Vector database storage. - <code>postgres_data/</code>: PostgreSQL database storage. - <code>ollama/</code>: Local LLM models.</p>"},{"location":"PROJECT_STRUCTURE/#key-configuration-files","title":"Key Configuration Files","text":"<ul> <li><code>.env</code>: Environment variables (API keys, ports, service URLs).</li> <li><code>pyproject.toml</code> / <code>requirements.txt</code>: Python dependency definitions (found in service subfolders).</li> <li><code>Dockerfile</code>: Build instructions for each service (found in service subfolders).</li> </ul>"},{"location":"RUNBOOK/","title":"Operations Runbook","text":"<p>Operational procedures for starting, stopping, validating, and maintaining OntoSage.</p>"},{"location":"RUNBOOK/#start-core-stack","title":"Start Core Stack","text":"<pre><code>docker-compose -f docker-compose.agentic.yml up -d --build orchestrator graphdb-rag-service code-executor whisper-stt frontend redis qdrant graphdb mysql\n</code></pre>"},{"location":"RUNBOOK/#stop-all","title":"Stop All","text":"<pre><code>docker-compose -f docker-compose.agentic.yml down\n</code></pre>"},{"location":"RUNBOOK/#health-checks","title":"Health Checks","text":"<pre><code>curl http://localhost:8000/health   # Orchestrator\ncurl http://localhost:8001/health   # RAG Service\ncurl http://localhost:8002/health   # Code Executor\ncurl http://localhost:3000          # Frontend\ncurl http://localhost:6333/health   # Qdrant\ncurl http://localhost:7200/rest/repositories  # GraphDB\n</code></pre>"},{"location":"RUNBOOK/#logs","title":"Logs","text":"<pre><code>docker-compose -f docker-compose.agentic.yml logs -f orchestrator\n</code></pre>"},{"location":"RUNBOOK/#backups","title":"Backups","text":"<ul> <li>Persistent data lives under <code>./volumes/*</code> and external <code>mysql-data</code>.</li> <li>Snapshot: <pre><code>Compress-Archive -Path .\\volumes -DestinationPath .\\backups\\ontosage-$(Get-Date -Format 'yyyyMMdd-HHmmss').zip\n</code></pre></li> </ul>"},{"location":"RUNBOOK/#gpu-model-management-ollama","title":"GPU Model Management (Ollama)","text":"<pre><code>docker exec ollama-deepseek-r1 ollama pull deepseek-r1:32b\n</code></pre>"},{"location":"RUNBOOK/#profiles","title":"Profiles","text":"<ul> <li>Enable monitoring: <pre><code>docker-compose -f docker-compose.agentic.yml --profile monitoring up -d prometheus grafana\n</code></pre></li> </ul>"},{"location":"RUNBOOK/#common-issues","title":"Common Issues","text":"<ul> <li>Port conflicts: adjust host ports in compose</li> <li>Slow first start: model downloads and initial GraphDB setup</li> <li>OOM: lower <code>OLLAMA_GPU_LAYERS</code> or use a smaller model</li> </ul>"},{"location":"SECURITY/","title":"Security &amp; Isolation","text":""},{"location":"SECURITY/#code-execution","title":"Code Execution","text":"<ul> <li>Executed in dedicated container with limited CPU/RAM</li> <li>No network access (recommended); only whitelisted paths</li> <li>Timeouts enforced via config (<code>CODE_EXECUTOR_TIMEOUT</code>)</li> </ul>"},{"location":"SECURITY/#secrets","title":"Secrets","text":"<ul> <li>Never commit <code>.env</code> with real keys</li> <li>Use environment variables and Docker secrets where possible</li> </ul>"},{"location":"SECURITY/#network","title":"Network","text":"<ul> <li>Prefer internal networks; restrict external binds to localhost</li> <li>GraphDB ports are bound to 127.0.0.1 by default in compose</li> </ul>"},{"location":"SECURITY/#authentication","title":"Authentication","text":"<ul> <li>API keys for Abacws API</li> <li>Optional auth for orchestrator endpoints (planned)</li> </ul>"},{"location":"SECURITY/#supply-chain","title":"Supply Chain","text":"<ul> <li>Pin base images when possible</li> <li>Rebuild regularly and keep images updated</li> </ul>"},{"location":"SERVICES/","title":"Service Catalog","text":"<p>This document describes every service in OntoSage 2.0, including responsibilities, ports, environment, health checks, dependencies, and interactions.</p>"},{"location":"SERVICES/#overview-diagram","title":"Overview Diagram","text":"<pre><code>graph LR\n  subgraph UI\n    FE[Frontend]\n  end\n  subgraph Core\n    ORCH[Orchestrator]\n    REDIS[(Redis)]\n  end\n  subgraph Knowledge\n    GDB[(GraphDB)]\n    QDR[Qdrant]\n    RAG[RAG Service]\n  end\n  subgraph Compute\n    EXEC[Code Executor]\n    WHIS[Whisper STT]\n    OLLA[Ollama]\n  end\n  subgraph Data\n    MYSQL[(MySQL)]\n    PGU[(Postgres User Data)]\n    MONGO[(Mongo Chat History)]\n    FILES[(Artifacts Nginx)]\n  end\n\n  FE -- REST/WS --&gt; ORCH\n  ORCH -- cache --&gt; REDIS\n  ORCH -- RAG ctx --&gt; RAG\n  RAG -- vectors --&gt; QDR\n  RAG -- SPARQL/REST --&gt; GDB\n  ORCH -- exec code --&gt; EXEC\n  ORCH -- STT --&gt; WHIS\n  ORCH -. inference .-&gt; OLLA\n  ORCH -- SQL --&gt; MYSQL\n  FE -. dashboards .-&gt; FILES\n  FE -. optional .-&gt; PGU\n  FE -. optional .-&gt; MONGO</code></pre>"},{"location":"SERVICES/#orchestrator-fastapi-langgraph","title":"Orchestrator (FastAPI + LangGraph)","text":"<ul> <li>Port: <code>8000</code></li> <li>Image/Build: <code>orchestrator/Dockerfile</code></li> <li>Mounts: <code>./orchestrator</code>, <code>./shared</code> (hot reload), <code>./data/bldg1</code>, <code>./outputs/query_results</code></li> <li>Env (key): <code>REDIS_HOST</code>, <code>RAG_SERVICE_HOST</code>, <code>CODE_EXECUTOR_HOST</code>, <code>WHISPER_STT_HOST</code>, <code>GRAPHDB_HOST</code>, <code>MYSQL_HOST</code>, model provider vars</li> <li>Health: <code>GET /health</code></li> <li>Depends: <code>redis</code>, <code>graphdb-rag-service</code>, <code>code-executor</code>, <code>graphdb</code>, <code>mysql</code>, <code>ollama</code></li> <li>Duties:</li> <li>Conversation state and tool routing</li> <li>Agent graph (Dialogue, SPARQL, SQL, Analytics, Visualization)</li> <li>Provider abstraction (Ollama/OpenAI)</li> <li>OpenAI-compatible <code>/v1</code> proxy (when enabled)</li> <li>API Standardization: All endpoints return:     <pre><code>{\n  \"success\": true,\n  \"data\": { ... },\n  \"error\": null,\n  \"meta\": null\n}\n</code></pre></li> </ul>"},{"location":"SERVICES/#rag-service-graphdb-qdrant","title":"RAG Service (GraphDB + Qdrant)","text":"<ul> <li>Port: <code>8001</code></li> <li>Image/Build: <code>rag-service/graphdbRAG/Dockerfile</code></li> <li>Env (key): <code>GRAPHDB_REPOSITORY</code>, <code>GRAPHDB_SIMILARITY_INDEX</code></li> <li>Health: <code>GET /health</code></li> <li>Depends: <code>graphdb</code></li> <li>Duties:</li> <li>Chunking and embedding ontology/documents</li> <li>Vector search in Qdrant; optional GraphDB similarity index</li> <li>Returns citations and entity IDs for grounding answers</li> </ul>"},{"location":"SERVICES/#graphdb","title":"GraphDB","text":"<ul> <li>Ports: <code>7200</code> (HTTP), <code>7300</code> (gRPC) [bound to localhost]</li> <li>Volumes: <code>./volumes/graphdb:/opt/graphdb/home</code>, <code>./data/bldg1/trial/dataset:/opt/graphdb/import:ro</code></li> <li>Env: <code>GDB_HEAP_SIZE</code>, <code>GDB_MAX_MEM</code></li> <li>Health: <code>GET /rest/repositories</code></li> <li>Duties: Ontology repository and SPARQL endpoint for building knowledge</li> </ul>"},{"location":"SERVICES/#qdrant","title":"Qdrant","text":"<ul> <li>Ports: <code>6333</code> (HTTP), <code>6334</code> (gRPC)</li> <li>Volume: <code>./volumes/qdrant:/qdrant/storage</code></li> <li>Health: <code>GET /health</code></li> <li>Duties: Vector embeddings store for RAG</li> </ul>"},{"location":"SERVICES/#code-executor","title":"Code Executor","text":"<ul> <li>Port: <code>8002</code></li> <li>Build: <code>code-executor/Dockerfile</code></li> <li>Env: from <code>.env</code> (timeout/limits via shared config)</li> <li>Health: <code>GET /health</code></li> <li>Duties: Sandboxed Python execution of Analytics Agent code</li> </ul>"},{"location":"SERVICES/#whisper-stt","title":"Whisper STT","text":"<ul> <li>Port: host <code>8003</code> -&gt; container <code>10300</code></li> <li>Image: <code>lscr.io/linuxserver/faster-whisper</code></li> <li>Env: <code>WHISPER_MODEL</code>, <code>WHISPER_BEAM</code>, <code>WHISPER_LANG</code></li> <li>Health: <code>GET /health</code> on <code>10300</code></li> <li>Duties: Audio transcription</li> </ul>"},{"location":"SERVICES/#ollama-local-llm","title":"Ollama (Local LLM)","text":"<ul> <li>Port: host <code>11435</code> -&gt; container <code>11434</code></li> <li>Volume: <code>./volumes/ollama:/root/.ollama</code></li> <li>Health: <code>ollama list</code></li> <li>Duties: Local model serving (e.g., <code>deepseek-r1:32b</code>)</li> </ul>"},{"location":"SERVICES/#redis-memory-store","title":"Redis (Memory Store)","text":"<ul> <li>Port: <code>6379</code></li> <li>Volume: <code>./volumes/redis</code></li> <li>Command: <code>--appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru</code></li> <li>Health: <code>redis-cli ping</code></li> <li>Duties: Conversation and agent state</li> </ul>"},{"location":"SERVICES/#mysql-sensor-time-series","title":"MySQL (Sensor Time-Series)","text":"<ul> <li>Port: host <code>3307</code> -&gt; container <code>3306</code></li> <li>Volume: <code>mysql-data:/var/lib/mysql</code> (external)</li> <li>Health: <code>mysqladmin ping</code></li> <li>Duties: Historical telemetry, sensor UUID mappings</li> </ul>"},{"location":"SERVICES/#postgres-user-data","title":"Postgres (User Data)","text":"<ul> <li>Port: host <code>5433</code> -&gt; container <code>5432</code></li> <li>Volume: <code>./volumes/postgres-user-data</code></li> <li>Health: <code>pg_isready</code></li> <li>Duties: User chat history, UI data (optional)</li> </ul>"},{"location":"SERVICES/#mongo-chat-history","title":"Mongo (Chat History)","text":"<ul> <li>Port: <code>27017</code></li> <li>Volume: <code>./volumes/mongo</code></li> <li>Health: <code>mongosh db.adminCommand('ping')</code></li> <li>Duties: Chat/session data for legacy UI and integrations</li> </ul>"},{"location":"SERVICES/#abacws-api-visualiser-legacyintegration","title":"Abacws API &amp; Visualiser (Legacy/Integration)","text":"<ul> <li>API Port: <code>5000</code>; Visualiser Port: <code>8090</code></li> <li>Build: <code>Abacws/api</code> and <code>Abacws/visualiser</code></li> <li>Duties: 3D digital twin and REST backend used by visualiser</li> </ul>"},{"location":"SERVICES/#file-server-artifacts","title":"File Server (Artifacts)","text":"<ul> <li>Port: <code>8080</code></li> <li>Volume: <code>./volumes/artifacts</code></li> <li>Duties: Serve generated plots/images/artifacts via static hosting</li> </ul>"},{"location":"SERVICES/#monitoring-optional","title":"Monitoring (Optional)","text":"<ul> <li>Prometheus: <code>9090</code>; Grafana: <code>3001</code></li> <li>Duties: Metrics collection and dashboards (enable with <code>--profile monitoring</code>)</li> </ul>"},{"location":"USER_GUIDE/","title":"User Guide","text":"<p>This guide explains how to interact with OntoSage 2.0 through the chat UI.</p>"},{"location":"USER_GUIDE/#getting-started","title":"Getting Started","text":"<ol> <li>Login: Use your credentials to access the dashboard.</li> <li>Select Persona: Choose a persona (Student, Researcher, Facility Manager) to tailor the AI's responses.</li> <li>Select Building: Pick the building you want to query from the dropdown.</li> </ol>"},{"location":"USER_GUIDE/#personas","title":"Personas","text":"<ul> <li>Student: The AI acts as a tutor, explaining concepts and terminology simply.</li> <li>Researcher: The AI provides detailed, ontology-driven explanations and data provenance.</li> <li>Facility Manager: The AI focuses on operational status, alarms, KPIs, and actionable insights.</li> </ul>"},{"location":"USER_GUIDE/#interaction-modes","title":"Interaction Modes","text":""},{"location":"USER_GUIDE/#chat","title":"\ud83d\udcac Chat","text":"<p>Type your questions naturally. The system understands context, so you can ask follow-up questions.</p> <p>Examples: *   General: \"Explain the HVAC system in Building 1\" *   Structural (SPARQL): \"List all CO\u2082 sensors on Floor 2\" *   Data (SQL): \"What was the average temperature in Room 101 last week?\" *   Analytics: \"Is there a correlation between occupancy and temperature in Room 101?\" *   Visualization: \"Plot hourly electricity usage for yesterday\"</p>"},{"location":"USER_GUIDE/#voice-input","title":"\ud83c\udfa4 Voice Input","text":"<ol> <li>Click the Microphone icon.</li> <li>Speak your query clearly (e.g., \"Show me the active alarms\").</li> <li>Click stop to send. The system will transcribe and process your request.</li> </ol>"},{"location":"USER_GUIDE/#3d-digital-twin","title":"\ud83c\udfe2 3D Digital Twin","text":"<p>The 3D viewer on the right side of the screen is interactive: *   Highlighting: When you ask about a specific room or sensor, the viewer will automatically zoom in and highlight it. *   Navigation: You can manually rotate, zoom, and pan to explore the building structure.</p>"},{"location":"USER_GUIDE/#tips-for-best-results","title":"Tips for Best Results","text":"<ul> <li>Be Specific: Mention specific room numbers (e.g., \"Room 101\") or equipment names if known.</li> <li>Time Ranges: For data queries, specify a time range (e.g., \"last 24 hours\", \"last month\").</li> <li>Follow-up: If an answer is incomplete, ask \"Can you elaborate?\" or \"Show me the data for the previous week too\".</li> </ul>"},{"location":"WORKFLOW/","title":"System Workflow Deep Dive","text":"<p>This document provides a comprehensive, \"under-the-hood\" explanation of the OntoSage 2.0 workflow. It details how requests are processed, how agents interact, and the specific code paths involved in generating answers.</p>"},{"location":"WORKFLOW/#1-request-lifecycle","title":"1. Request Lifecycle","text":"<p>The journey of a user request follows this path:</p> <ol> <li>Frontend: User sends a message via the React UI (<code>/chat</code> endpoint).</li> <li>Orchestrator (FastAPI):<ul> <li>Receives the request in <code>orchestrator/main.py</code>.</li> <li>Validates authentication (<code>auth_manager.py</code>).</li> <li>Loads/Creates <code>ConversationState</code> from Redis.</li> <li>Passes the state to <code>WorkflowOrchestrator</code>.</li> </ul> </li> <li>LangGraph Execution:<ul> <li>The <code>WorkflowOrchestrator</code> (<code>orchestrator/workflow.py</code>) initializes the state graph.</li> <li>Execution starts at the Dialogue Node.</li> </ul> </li> <li>Agent Routing:<ul> <li>Dialogue Agent analyzes the intent (e.g., <code>sparql</code>, <code>sql</code>, <code>analytics</code>, <code>general</code>).</li> <li>The graph routes the state to the appropriate specialized agent.</li> </ul> </li> <li>Task Execution:<ul> <li>The specialized agent performs its task (querying DB, running code, etc.).</li> <li>Results are stored in <code>state.intermediate_results</code>.</li> </ul> </li> <li>Response Generation:<ul> <li>The flow returns to the Response Node (often back to Dialogue Agent or a dedicated response generator).</li> <li>The LLM synthesizes a final natural language answer using the intermediate results.</li> </ul> </li> <li>Delivery:<ul> <li>The final response is saved to Redis/Postgres.</li> <li>The Orchestrator returns a standardized <code>APIResponse</code> JSON to the frontend.</li> </ul> </li> </ol>"},{"location":"WORKFLOW/#2-langgraph-orchestration","title":"2. LangGraph Orchestration","text":"<p>The core logic is defined in <code>orchestrator/workflow.py</code>. We use LangGraph to define a state machine where nodes are agents and edges are routing logic.</p>"},{"location":"WORKFLOW/#the-graph-structure","title":"The Graph Structure","text":"<pre><code># orchestrator/workflow.py\n\ndef _build_graph(self) -&gt; StateGraph:\n    workflow = StateGraph(ConversationState)\n\n    # Nodes\n    workflow.add_node(\"dialogue\", self._dialogue_node)\n    workflow.add_node(\"sparql\", self._sparql_node)\n    workflow.add_node(\"sql\", self._sql_node)\n    workflow.add_node(\"analytics\", self._analytics_node)\n    workflow.add_node(\"visualization\", self._visualization_node)\n    workflow.add_node(\"response\", self._response_node)\n\n    # Entry Point\n    workflow.set_entry_point(\"dialogue\")\n\n    # Conditional Edges (Routing)\n    workflow.add_conditional_edges(\n        \"dialogue\",\n        self._route_from_dialogue,\n        {\n            \"sparql\": \"sparql\",\n            \"sql\": \"sql\",\n            \"analytics\": \"analytics\",\n            \"visualization\": \"visualization\",\n            \"response\": \"response\",\n            \"end\": END\n        }\n    )\n    # ... (other edges)\n</code></pre>"},{"location":"WORKFLOW/#conversation-state","title":"Conversation State","text":"<p>The state passed between agents is defined in <code>shared/models.py</code>:</p> <pre><code>class ConversationState(BaseModel):\n    conversation_id: str\n    user_message: str\n    messages: List[Message]\n    current_intent: Optional[str]\n    intermediate_results: Dict[str, Any]  # Stores output from SPARQL/SQL agents\n    analytics_required: bool\n    # ...\n</code></pre>"},{"location":"WORKFLOW/#3-agent-internals","title":"3. Agent Internals","text":""},{"location":"WORKFLOW/#a-dialogue-agent-orchestratoragentsdialogue_agentpy","title":"A. Dialogue Agent (<code>orchestrator/agents/dialogue_agent.py</code>)","text":"<ul> <li>Role: The \"Front Desk\". It classifies intent and handles general chit-chat.</li> <li>Mechanism:<ol> <li>Retrieves context from RAG Service (<code>_retrieve_ontology_context</code>).</li> <li>Constructs a prompt with conversation history and retrieved context.</li> <li>Asks the LLM to classify the intent into: <code>sparql</code>, <code>sql</code>, <code>analytics</code>, <code>visualization</code>, or <code>general</code>.</li> </ol> </li> <li>Code Highlight:     <pre><code># Intent Classification Prompt\nprompt = f\"\"\"\nAnalyze the user's request: \"{state.user_message}\"\nDetermine the best tool to use:\n- SPARQL: For questions about building structure, sensors, rooms, or metadata.\n- SQL: For questions about historical sensor data, temperature readings, energy usage.\n- ANALYTICS: For statistical analysis, correlations, or complex data processing.\n- GENERAL: For greetings, clarifications, or general knowledge.\n\"\"\"\n</code></pre></li> </ul>"},{"location":"WORKFLOW/#b-sparql-agent-orchestratoragentssparql_agentpy","title":"B. SPARQL Agent (<code>orchestrator/agents/sparql_agent.py</code>)","text":"<ul> <li>Role: Queries the Ontology (GraphDB).</li> <li>Workflow:<ol> <li>Schema Retrieval: Fetches relevant schema parts (classes, properties) from RAG.</li> <li>Query Generation: Uses LLM to generate a SPARQL query based on the user question and schema.</li> <li>Execution: Sends the query to GraphDB via HTTP.</li> <li>Caching: Checks Redis for cached results of identical semantic queries.</li> </ol> </li> <li>Key File: <code>orchestrator/agents/sparql_agent.py</code></li> </ul>"},{"location":"WORKFLOW/#c-sql-agent-orchestratoragentssql_agentpy","title":"C. SQL Agent (<code>orchestrator/agents/sql_agent.py</code>)","text":"<ul> <li>Role: Queries Time-Series Data (MySQL).</li> <li>Security: Implements Strict SQL Validation to prevent injection.     <pre><code>def validate_sql(self, query: str) -&gt; bool:\n    # Only allow SELECT\n    if not re.match(r\"^\\s*SELECT\", query, re.IGNORECASE):\n        return False\n    # Block DML/DDL\n    if re.search(r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER)\\b\", query, re.IGNORECASE):\n        return False\n    return True\n</code></pre></li> <li>Workflow: Similar to SPARQL Agent but targets MySQL tables (<code>sensor_data</code>, <code>devices</code>).</li> </ul>"},{"location":"WORKFLOW/#d-analytics-agent-orchestratoragentsanalytics_agentpy","title":"D. Analytics Agent (<code>orchestrator/agents/analytics_agent.py</code>)","text":"<ul> <li>Role: Performs data analysis.</li> <li>Mechanism:<ol> <li>Template Matching: Checks if the request matches a pre-defined template (e.g., \"correlation between X and Y\").</li> <li>Code Generation: If matched, uses a deterministic Python template. If not, uses LLM to generate Python code.</li> <li>Execution: Sends the code to the Code Executor service (Docker sandbox).</li> <li>Result: Returns text output or a path to a generated plot image.</li> </ol> </li> </ul>"},{"location":"WORKFLOW/#4-rag-service-workflow","title":"4. RAG Service Workflow","text":"<p>The RAG Service (<code>rag-service/</code>) bridges the gap between unstructured text and structured knowledge.</p> <ol> <li>Ingestion:<ul> <li>Ontology files (<code>.ttl</code>) and documents (<code>.md</code>, <code>.pdf</code>) are chunked.</li> <li>Embeddings are generated using <code>sentence-transformers/all-MiniLM-L6-v2</code>.</li> <li>Vectors are stored in Qdrant.</li> </ul> </li> <li>Retrieval:<ul> <li>User query is embedded.</li> <li>Qdrant performs a vector similarity search.</li> <li>Top-k results (text chunks + entity URIs) are returned to the Orchestrator.</li> </ul> </li> </ol>"},{"location":"WORKFLOW/#5-data-state-management","title":"5. Data &amp; State Management","text":""},{"location":"WORKFLOW/#redis-caching-orchestratorredis_managerpy","title":"Redis Caching (<code>orchestrator/redis_manager.py</code>)","text":"<ul> <li>Conversation State: Persisted after every turn.</li> <li>Semantic Caching:<ul> <li>Key: <code>hash(intent + canonical_query)</code></li> <li>Value: JSON result from DB.</li> <li>TTL: 1 hour.</li> <li>Benefit: drastically reduces latency for repeated questions like \"What is the temperature?\".</li> </ul> </li> </ul>"},{"location":"WORKFLOW/#postgres-persistence-orchestratorpostgres_managerpy","title":"Postgres Persistence (<code>orchestrator/postgres_manager.py</code>)","text":"<ul> <li>Used for long-term storage of user accounts, conversation history, and audit logs.</li> <li>Acts as the \"Source of Truth\" if Redis is flushed.</li> </ul>"},{"location":"WORKFLOW/#6-api-standardization","title":"6. API Standardization","text":"<p>All endpoints in <code>orchestrator/main.py</code> follow this response format:</p> <pre><code>{\n  \"success\": true,  // or false\n  \"data\": {         // The actual payload\n    \"conversation_id\": \"...\",\n    \"response\": \"...\",\n    \"intent\": \"sql\"\n  },\n  \"error\": null,    // Error message if success is false\n  \"meta\": {         // Debug info, timing, etc.\n    \"cached\": true\n  }\n}\n</code></pre> <p>This ensures the frontend can consistently handle success and error states.</p>"}]}