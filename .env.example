# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                OntoSage 2.0 Environment Configuration                      â•‘
# â•‘                                                                            â•‘
# â•‘  QUICK START:                                                             â•‘
# â•‘  1. For FREE local models:  Copy .env.local to .env                       â•‘
# â•‘  2. For PAID cloud models:  Copy .env.cloud to .env and add OpenAI key   â•‘
# â•‘  3. Run: ./startup.ps1                                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# ==================== ğŸ¯ MODEL PROVIDER SELECTION ====================
# Core decision: Where do models run?
#
# LOCAL OPTION (Recommended for testing):
#   - MODEL_PROVIDER=local
#   - EMBEDDING_PROVIDER=local
#   - STT_PROVIDER=local
#   - âœ… FREE (no API costs)
#   - âœ… Privacy (data stays local)
#   - âœ… No rate limits
#   - âš ï¸ Slower inference
#   - âš ï¸ Requires 8GB+ RAM
#   - Uses: Ollama (mistral), sentence-transformers, faster-whisper
#
# CLOUD OPTION (Recommended for production):
#   - MODEL_PROVIDER=openai
#   - EMBEDDING_PROVIDER=openai
#   - STT_PROVIDER=openai
#   - âœ… Fast inference
#   - âœ… State-of-the-art models
#   - âš ï¸ Costs money (~$0.01 per 1K tokens)
#   - âš ï¸ Requires internet
#   - âš ï¸ Needs OpenAI API key
#   - Uses: GPT-4, ada-002, Whisper API

MODEL_PROVIDER=local
EMBEDDING_PROVIDER=local
STT_PROVIDER=local


# ==================== â˜ï¸ OPENAI CONFIGURATION ====================
# Required only if MODEL_PROVIDER=openai, EMBEDDING_PROVIDER=openai, or STT_PROVIDER=openai
#
# Get your API key from: https://platform.openai.com/api-keys
# Pricing: https://openai.com/pricing
#
# SECURITY WARNING:
# - Never commit .env with real API keys to Git
# - Rotate keys if exposed
# - Monitor usage: https://platform.openai.com/usage

# Your OpenAI API key (starts with sk-...)
OPENAI_API_KEY=your_openai_api_key_here

# LLM Model for conversations, SPARQL generation, analytics
# Options:
#   - gpt-4-turbo-preview (best quality, $0.01/1K tokens input, $0.03/1K output)
#   - gpt-4 (proven stable, $0.03/1K input, $0.06/1K output)
#   - gpt-3.5-turbo (cheapest, $0.0005/1K input, $0.0015/1K output)
OPENAI_MODEL=gpt-4-turbo-preview

# Temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
# For ontology queries: 0.0-0.2 recommended (precise)
# For conversations: 0.5-0.7 (natural)
OPENAI_TEMPERATURE=0.1

# Embedding model for RAG (Retrieval-Augmented Generation)
# Options:
#   - text-embedding-3-small (recommended, 1536 dims, $0.00002/1K tokens)
#   - text-embedding-3-large (best quality, 3072 dims, $0.00013/1K tokens)
#   - text-embedding-ada-002 (legacy, 1536 dims, $0.0001/1K tokens)
EMBEDDING_MODEL_OPENAI=text-embedding-3-small
EMBEDDING_DIMENSION_OPENAI=1536

# Whisper STT (Speech-to-Text)
# Fixed model: whisper-1 ($0.006/minute)
# Max file size: 25MB
# Supports: mp3, mp4, mpeg, mpga, m4a, wav, webm


# ==================== ğŸ–¥ï¸ LOCAL MODEL CONFIGURATION ====================
# Used only if MODEL_PROVIDER=local, EMBEDDING_PROVIDER=local, or STT_PROVIDER=local

# Ollama (Local LLM)
# URL points to Ollama container (do not change unless using external Ollama)
OLLAMA_BASE_URL=http://ollama:11434

# Model to use (must be pulled first: docker exec ollama-local-llm ollama pull mistral)
# Options:
#   - mistral:latest (7B params, good quality, 4GB RAM)
#   - llama3:latest (8B params, better quality, 5GB RAM)
#   - phi3:latest (3.8B params, faster, lower RAM)
#   - mixtral:latest (47B params, best quality, 26GB RAM)
OLLAMA_MODEL=mistral:latest

# Local Embeddings (sentence-transformers)
# Model is downloaded on first use (~90MB)
# Options:
#   - sentence-transformers/all-MiniLM-L6-v2 (384 dims, fast, good quality)
#   - sentence-transformers/all-mpnet-base-v2 (768 dims, slower, better quality)
#   - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (384 dims, multilingual)
EMBEDDING_MODEL_LOCAL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSION_LOCAL=384

# Local Whisper (faster-whisper)
# Model size affects accuracy vs speed (download size shown)
# Options:
#   - tiny (39MB, fast, low accuracy)
#   - base (74MB, balanced, recommended)
#   - small (244MB, good accuracy)
#   - medium (769MB, better accuracy)
#   - large (2.9GB, best accuracy, slow)
WHISPER_MODEL_LOCAL=base


# ==================== ğŸ”— SERVICE URLS ====================
# Internal Docker network URLs (do not change unless modifying docker-compose.yml)

# Qdrant Vector Database (semantic search)
# Stores ontology embeddings, query history
# Dashboard: http://localhost:6333/dashboard
QDRANT_URL=http://qdrant:6333

# Redis (conversation state, caching)
# Stores conversation history, SPARQL cache, user preferences
# TTL: CONVERSATION_TTL seconds (default: 3600 = 1 hour)
REDIS_URL=redis://redis:6379/0

# RAG Service (Retrieval-Augmented Generation)
# Provides ontology context for SPARQL generation
# Health check: http://localhost:8001/health
RAG_SERVICE_URL=http://rag-service:8001

# Code Executor (Sandboxed Python execution)
# Runs analytics code in isolated environment
# Security: SAFE_BUILTINS only, no os/sys/subprocess
CODE_EXECUTOR_URL=http://code-executor:8002
CODE_EXECUTOR_TIMEOUT=30              # Max execution time (seconds)
CODE_EXECUTOR_MEMORY_LIMIT=1g         # RAM limit per execution
CODE_EXECUTOR_CPU_LIMIT=1.0           # CPU cores (1.0 = 1 full core)

# Whisper STT (Speech-to-Text)
# Converts audio to text for voice queries
WHISPER_STT_URL=http://whisper-stt:8003


# ==================== ğŸ¢ BUILDING 1 CONFIGURATION ====================
# Building metadata and data source endpoints

# Building identifier (used in logs, responses)
BUILDING_ID=bldg1
BUILDING_NAME=Abacws Building

# Fuseki SPARQL Endpoint
# Triple store for Brick ontology (Building 1 sensors, locations, equipment)
# Query UI: http://localhost:3030/#/dataset/abacws/query
# Dataset contains:
#   - ~500 triples (sensors, spaces, equipment, relationships)
#   - Brick Schema classes (hasLocation, feeds, isPointOf, etc.)
FUSEKI_URL=http://fuseki:3030/abacws

# MySQL Database (Time-series sensor data)
# Contains:
#   - sensors table: 21 sensors (7 temp, 5 humidity, 3 CO2, 3 occupancy, 3 energy)
#   - sensor_data: ~14K records (7 days, 15-minute intervals)
#   - sensor_hourly_aggregates: Pre-computed hourly stats
# PhpMyAdmin (optional): http://localhost:8080
MYSQL_HOST=mysql
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=root                   # âš ï¸ Change in production!
MYSQL_DATABASE=abacws

# PostgreSQL Database (ThingsBoard)
# Contains:
#   - ThingsBoard device registry, telemetry, and time-series data
#   - Used for IoT device management and sensor data storage
# Access via pgAdmin: http://localhost:5050
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=thingsboard
POSTGRES_PASSWORD=thingsboard         # âš ï¸ Change in production!
POSTGRES_DB=thingsboard

# pgAdmin (PostgreSQL Web UI)
# Web interface for managing PostgreSQL databases
# Access: http://localhost:5050
# Login with credentials below
PGADMIN_DEFAULT_EMAIL=pgadmin@example.com
PGADMIN_DEFAULT_PASSWORD=admin        # âš ï¸ Change in production!


# ==================== ğŸ—ï¸ ABACWS API & VISUALISER CONFIGURATION ====================
# Building management API with support for multiple database backends

# API Key for authentication
API_KEY=change_me                     # âš ï¸ Change in production!

# Database Engine Selection
# Options: mysql, postgres, mongo, disabled
API_DB_ENGINE=mysql

# Abacws API PostgreSQL Connection (when API_DB_ENGINE=postgres)
API_PGHOST=postgres
API_PGPORT=5432
API_PGUSER=thingsboard
API_PGPASSWORD=thingsboard
API_PGDATABASE=thingsboard

# Abacws API MySQL Connection (when API_DB_ENGINE=mysql)
API_MYSQL_HOST=mysql-bldg1
API_MYSQL_PORT=3307
API_MYSQL_USER=root
API_MYSQL_PASSWORD=mysql
API_MYSQL_DATABASE=sensordb

# Abacws Visualiser Debug Mode
# Set to '1' to enable verbose 3D loader logging in browser console
ABACWS_DEBUG=0


# ==================== ğŸŒ THINGSBOARD IOT PLATFORM CONFIGURATION ====================
# ThingsBoard for sensor management and IoT device connectivity

# ThingsBoard PostgreSQL Database
# Uses the same Postgres container as other services
PG_THINGSBOARD_DB=thingsboard
PG_THINGSBOARD_USER=thingsboard
PG_THINGSBOARD_PASSWORD=thingsboard

# ThingsBoard Access
# Web UI: http://localhost:8082
# Default credentials: tenant@thingsboard.org / tenant
# MQTT Port: 1883
# HTTP Port: 8081


# ==================== ğŸ’¬ CONVERSATION SETTINGS ====================

# Conversation state TTL (seconds)
# How long Redis keeps conversation history before deletion
# 3600 = 1 hour, 86400 = 24 hours
CONVERSATION_TTL=3600

# Max messages stored per conversation
# Older messages are removed (sliding window)
# Trade-off: More context vs memory usage
# 20 = 10 user + 10 assistant messages
MAX_CONVERSATION_HISTORY=20

# Max retry attempts for failed operations
# Applies to:
#   - SPARQL query errors (auto-repair)
#   - Code execution errors (auto-fix)
#   - LLM API failures (retry with backoff)
MAX_RETRY_ATTEMPTS=3


# ==================== ğŸ“Š QDRANT COLLECTION CONFIGURATION ====================
# Vector database collections (created by scripts/init_qdrant.py)

# Collection names
QDRANT_ONTOLOGY_COLLECTION=ontology   # Brick Schema triples
QDRANT_QUERIES_COLLECTION=queries     # Historical SPARQL queries
QDRANT_ANALYTICS_COLLECTION=analytics # Analytics code snippets
QDRANT_DOCS_COLLECTION=documentation  # System docs, help text

# Vector similarity threshold (0.0-1.0)
# Higher = stricter matching (fewer, more relevant results)
# Lower = looser matching (more results, less relevant)
# 0.7 = good balance
SIMILARITY_THRESHOLD=0.7

# Max results returned per RAG query
# More results = more context for LLM but slower inference
RAG_TOP_K=5


# ==================== ğŸ” SECURITY SETTINGS ====================

# Code Executor Sandbox
# Forbidden imports (security): os, sys, subprocess, socket, requests, urllib
# Allowed imports: pandas, numpy, matplotlib, plotly, datetime, math, statistics
# All code runs in isolated environment with resource limits

# File upload limits (Whisper STT)
MAX_UPLOAD_SIZE_MB=25                 # OpenAI Whisper limit
ALLOWED_AUDIO_FORMATS=wav,mp3,m4a,ogg,webm


# ==================== ğŸ“ LOGGING ====================

# Log level controls verbosity
# Options:
#   - DEBUG: Everything (function calls, variables, SQL queries)
#   - INFO: Important events (requests, responses, errors)
#   - WARNING: Potential issues (retries, timeouts)
#   - ERROR: Failures only
# Production: INFO, Development: DEBUG
LOG_LEVEL=INFO

# Log format (Python logging)
# %(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s


# ==================== ğŸ¨ FRONTEND CONFIGURATION ====================

# React app proxies /api/* to orchestrator
# Set via REACT_APP_API_URL in frontend/.env
# Default: http://localhost:8000 (local development)
# Production: Update to public orchestrator URL


# ==================== ğŸ“ˆ MONITORING (OPTIONAL) ====================
# Enable with: docker-compose --profile monitoring up -d

# Prometheus (metrics)
# Scrapes: orchestrator, rag-service, code-executor, whisper-stt, redis, qdrant, mysql
# UI: http://localhost:9090

# Grafana (dashboards)
# UI: http://localhost:3001
# Login: admin/admin
# Dashboards: OntoSage Overview (service health, latency, throughput)


# ==================== ğŸ› ï¸ DEVELOPMENT SETTINGS ====================

# Hot reload (for development)
# Set to 'true' to enable auto-restart on code changes
# Requires mounting code as volumes in docker-compose.yml
# DEV_MODE=false

# Skip model downloads (if already cached)
# SKIP_MODEL_DOWNLOAD=false

# Use mock LLM responses (for testing without LLM)
# MOCK_LLM=false


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  TROUBLESHOOTING                                                          â•‘
# â•‘                                                                            â•‘
# â•‘  â€¢ Services not starting?                                                 â•‘
# â•‘    â†’ Run: docker-compose logs <service_name>                             â•‘
# â•‘    â†’ Check: Docker has 8GB+ RAM allocated (Docker Desktop settings)      â•‘
# â•‘                                                                            â•‘
# â•‘  â€¢ Ollama model not found?                                                â•‘
# â•‘    â†’ Run: docker exec ollama-local-llm ollama pull mistral               â•‘
# â•‘                                                                            â•‘
# â•‘  â€¢ OpenAI API errors?                                                     â•‘
# â•‘    â†’ Verify: OPENAI_API_KEY is valid                                     â•‘
# â•‘    â†’ Check: https://platform.openai.com/usage for rate limits           â•‘
# â•‘                                                                            â•‘
# â•‘  â€¢ Qdrant collections not found?                                          â•‘
# â•‘    â†’ Run: docker-compose exec orchestrator python -m scripts.init_qdrant â•‘
# â•‘                                                                            â•‘
# â•‘  â€¢ No ontology data?                                                      â•‘
# â•‘    â†’ Run: docker-compose exec rag-service python -m scripts.ingest_ontology â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
