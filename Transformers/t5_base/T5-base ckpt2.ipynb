{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / upgrade core dependencies (safe to re-run). Comment out versions to use latest.\n",
    "%pip install \\\n",
    "  transformers==4.44.2 \\\n",
    "  datasets==2.21.0 \\\n",
    "  evaluate==0.4.2 \\\n",
    "  accelerate==0.34.2 \\\n",
    "  sentencepiece==0.2.0 \\\n",
    "  nltk==3.9.1 \\\n",
    "  rouge-score==0.1.2 \\\n",
    "  bert-score==0.3.13 \\\n",
    "  sacrebleu==2.4.3 \\\n",
    "  scikit-learn==1.5.2 \\\n",
    "  pandas==2.2.2 \\\n",
    "  numpy==1.26.4 \\\n",
    "  torch --upgrade --quiet\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "print('Dependencies installed and punkt tokenizer downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment & Dependencies\n",
    "\n",
    "Run the next cell once per environment to ensure all required packages (with tested versions) are installed before training/fine-tuning.\n",
    "\n",
    "Core libraries:\n",
    "- transformers (seq2seq model + trainer)\n",
    "- datasets (dataset handling)\n",
    "- evaluate (metrics hub)\n",
    "- accelerate (efficient distributed / mixed precision)\n",
    "- sentencepiece (tokenization backend for T5)\n",
    "- nltk (optional: sentence tokenization, metrics)\n",
    "- rouge_score, bert_score, sacrebleu (evaluation)\n",
    "- scikit-learn (train/validation split)\n",
    "- numpy, pandas (general utilities)\n",
    "\n",
    "Pin versions if you need reproducibility; here we choose recent stable versions that work well with T5-base. Adjust as needed for CUDA / platform constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued Fine-Tuning: Combined Extended + Schema Datasets\n",
    "\n",
    "This section loads the two newly generated datasets:\n",
    "- `bldg3_dataset_extended.json` (semantic sensor + analytic queries)\n",
    "- `Transformers/t5_base/training/bldg3/bldg3_schema_dataset.json` (ontology/TBox + structural queries)\n",
    "\n",
    "It merges them into a single training corpus, converts multi-entity lists into a flat string for the model input, and continues fine-tuning from the latest prior checkpoint (if available) or the base `t5-base` model. New checkpoints will be written under `./trained/combined_t5/`.\n",
    "\n",
    "You can adjust hyperparameters (epochs, batch size, lr) in the following cells if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dependencies installed and punkt tokenizer downloaded.\n",
      "Dependencies installed and punkt tokenizer downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Install / upgrade core dependencies (safe to re-run). Comment out versions to use latest.\n",
    "%pip install \\\n",
    "  transformers==4.44.2 \\\n",
    "  datasets==2.21.0 \\\n",
    "  evaluate==0.4.2 \\\n",
    "  accelerate==0.34.2 \\\n",
    "  sentencepiece==0.2.0 \\\n",
    "  nltk==3.9.1 \\\n",
    "  rouge-score==0.1.2 \\\n",
    "  bert-score==0.3.13 \\\n",
    "  sacrebleu==2.4.3 \\\n",
    "  scikit-learn==1.5.2 \\\n",
    "  pandas==2.2.2 \\\n",
    "  numpy==1.26.4 \\\n",
    "  torch --upgrade --quiet\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "print('Dependencies installed and punkt tokenizer downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: bert-score in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from datasets) (3.12.15)\n",
      "Requirement already satisfied: absl-py in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from rouge-score) (2.3.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from bert-score) (2.8.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from bert-score) (3.10.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from matplotlib->bert-score) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from matplotlib->bert-score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from matplotlib->bert-score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from matplotlib->bert-score) (3.2.5)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\suhas\\documents\\github\\ontobot\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (311)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate rouge-score sacrebleu nltk bert-score scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformers/t5_base/T5-base.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test on a couple of random samples after training (run after previous cell finishes)\n",
    "import random, json, torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = './trained/checkpoint-3/final'\n",
    "if not Path(model_dir).exists():\n",
    "    print('Final model directory not found, please run the training cell first.')\n",
    "else:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    with open('merged_combined_corpus.json','r',encoding='utf-8') as f:\n",
    "        merged = json.load(f)\n",
    "\n",
    "    samples = random.sample(merged, k=min(3, len(merged)))\n",
    "    for i,s in enumerate(samples,1):\n",
    "        inp = s['input_text']\n",
    "        tgt = s['target_text']\n",
    "        inputs = tokenizer(inp, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(device)\n",
    "        out_ids = model.generate(inputs['input_ids'], max_length=256, num_beams=4)\n",
    "        pred = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "        print(f'--- Sample {i} ---')\n",
    "        print('INPUT:\\n', inp[:400])\n",
    "        print('TARGET:\\n', tgt)\n",
    "        print('PRED:\\n', pred)\n",
    "        print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch: Safe decoding & debug utilities to prevent IndexError during evaluation\n",
    "import torch, json, math, statistics\n",
    "from typing import List\n",
    "\n",
    "# 1. Verify tokenizer/model alignment again\n",
    "try:\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    vocab_size = len(tokenizer)\n",
    "    print(f\"[DEBUG] Embedding size: {embedding_size} | Tokenizer vocab: {vocab_size}\")\n",
    "    if embedding_size != vocab_size:\n",
    "        print(\"[WARNING] Mismatch detected. This can cause out-of-range token ids during decode.\")\n",
    "        print(\"Suggested fix: Reload both model and tokenizer from the SAME checkpoint or resize embeddings.\")\n",
    "except Exception as e:\n",
    "    print(f\"[DEBUG] Could not inspect model/tokenizer sizes: {e}\")\n",
    "\n",
    "# 2. Utility to clamp OOB ids BEFORE batch_decode\n",
    "\n",
    "def clamp_oob_ids(seqs: List[List[int]], vocab_limit: int) -> int:\n",
    "    \"\"\"Clamp any token id >= vocab_limit or < 0 to pad_token id; returns count of corrections.\"\"\"\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    fixes = 0\n",
    "    for s in seqs:\n",
    "        for i, tid in enumerate(s):\n",
    "            if tid >= vocab_limit or tid < 0:\n",
    "                s[i] = pad_id\n",
    "                fixes += 1\n",
    "    return fixes\n",
    "\n",
    "# 3. Wrap original compute_metrics if not already wrapped\n",
    "if 'original_compute_metrics' not in globals():\n",
    "    original_compute_metrics = compute_metrics  # preserve\n",
    "\n",
    "    def safe_compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        # preds may be a tuple when using generate with return dict\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        preds_list = preds.tolist() if isinstance(preds, torch.Tensor) else preds\n",
    "        # Clamp out-of-range token ids\n",
    "        vocab_limit = len(tokenizer)\n",
    "        fix_count = clamp_oob_ids(preds_list, vocab_limit)\n",
    "        if fix_count:\n",
    "            print(f\"[SAFE-DECODE] Corrected {fix_count} out-of-range token ids before decoding.\")\n",
    "        # Convert back to tensor for downstream code expecting tensor shape\n",
    "        preds_tensor = torch.tensor(preds_list, device=preds.device if isinstance(preds, torch.Tensor) else 'cpu')\n",
    "        return original_compute_metrics((preds_tensor, labels))\n",
    "\n",
    "    compute_metrics = safe_compute_metrics\n",
    "    trainer.compute_metrics = compute_metrics\n",
    "    print(\"[PATCH] Installed safe_compute_metrics wrapper to prevent IndexError.\")\n",
    "else:\n",
    "    print(\"[PATCH] safe_compute_metrics already installed; skipping re-wrap.\")\n",
    "\n",
    "# 4. Optional immediate evaluation test (uncomment to dry-run metrics without full training)\n",
    "# try:\n",
    "#     print(\"[DRY-RUN] Starting quick evaluation to test decoding safety...\")\n",
    "#     trainer.evaluate(max_length=64)\n",
    "#     print(\"[DRY-RUN] Evaluation successful.\")\n",
    "# except Exception as e:\n",
    "#     print(\"[DRY-RUN] Evaluation failed:\", e)\n",
    "\n",
    "print(\"Patch cell executed. You can now re-run training: trainer.train() (ideally from the cell above).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Fallback Explanation\n",
    "If `evaluate.load('rouge')` fails (e.g., offline environment, hub connectivity, or cache corruption), the previous cell now:\n",
    "\n",
    "- Tries to `pip install` a required package (e.g. `rouge-score`, `sacrebleu`, `nltk`, `bert-score`).\n",
    "- If that still fails, it uses a lightweight local fallback implementation for: ROUGE (rouge-score), BLEU (sacrebleu), METEOR (nltk), BERTScore (bert-score).\n",
    "- Prints which metrics are using fallbacks.\n",
    "\n",
    "Usage Notes:\n",
    "- To force re-download from Hugging Face Hub later, ensure network is available and re-run the cell.\n",
    "- If you want to skip a metric entirely, you can comment out its block in `compute_metrics`.\n",
    "- Starting training: uncomment `trainer.train()` at the bottom of the previous cell.\n",
    "\n",
    "Troubleshooting:\n",
    "- If BERTScore is slow on CPU, you can remove it or set a smaller batch by modifying the fallback.\n",
    "- For deterministic runs, set `seed` in `Seq2SeqTrainingArguments` and also `random.seed`, `numpy.random.seed`, `torch.manual_seed` before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unified Single-Cell Training Pipeline (Extended + Schema, Multi-Building) ===\n",
    "# This cell can be run standalone after opening the notebook. It will:\n",
    "#  1. Ensure core dependencies are present.\n",
    "#  2. Load merged extended + schema datasets (with provenance).\n",
    "#  3. Normalize and assemble NL->SPARQL training pairs.\n",
    "#  4. Train/validation split.\n",
    "#  5. Load tokenizer & model from latest checkpoint (fallback to t5-base).\n",
    "#  6. Install robust metric fallback system (ROUGE, BLEU, METEOR, BERTScore).\n",
    "#  7. Patch compute_metrics with safe decode guard against out-of-range ids.\n",
    "#  8. Train with early stopping & automatic best-model selection.\n",
    "#  9. Save final model + tokenizer.\n",
    "# 10. Run a quick post-training generation smoke test.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, sys, json, random, gc, math, statistics, traceback, subprocess\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "SEED = 123\n",
    "EXTENDED_PATH = Path('./training/merged_extended_datasets.json')\n",
    "SCHEMA_PATH   = Path('./training/merged_schema_datasets.json')\n",
    "OUTPUT_BASE   = Path('./trained/unified_run')  # new consolidated output dir\n",
    "PREV_CHECKPOINT_CANDIDATES = [\n",
    "    './trained/checkpoint-2',\n",
    "]\n",
    "BASE_MODEL_NAME = 't5-base'\n",
    "TRAIN_VAL_SPLIT = 0.05\n",
    "TARGET_EPOCHS   = 5\n",
    "LEARNING_RATE   = 3e-4\n",
    "EVAL_STRATEGY   = 'epoch'\n",
    "BATCH_PER_DEVICE = 4\n",
    "GRAD_ACC_STEPS   = 4\n",
    "MAX_SOURCE_LEN = 512\n",
    "MAX_TARGET_LEN = 256\n",
    "EARLY_STOP_PATIENCE = 3\n",
    "METRIC_FOR_BEST = 'rougeL'\n",
    "SAVE_TOTAL_LIMIT = 3\n",
    "GEN_MAX_LEN = 512\n",
    "GEN_NUM_BEAMS = 1  # greedy-like (beam=1) for stability\n",
    "USE_FP16 = True  # auto-disabled if no CUDA\n",
    "\n",
    "# ---------------- Reproducibility ----------------\n",
    "random.seed(SEED)\n",
    "try:\n",
    "    import numpy as np\n",
    "    np.random.seed(SEED)\n",
    "except Exception:\n",
    "    pass\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---------------- Dependency Check (minimal) ----------------\n",
    "REQUIRED = [\n",
    "    ('transformers', 'transformers'),\n",
    "    ('datasets', 'datasets'),\n",
    "    ('evaluate', 'evaluate'),\n",
    "    ('sentencepiece', 'sentencepiece'),\n",
    "    ('nltk', 'nltk'),\n",
    "    ('sacrebleu', 'sacrebleu'),\n",
    "    ('rouge_score', 'rouge-score'),\n",
    "    ('bert_score', 'bert-score'),\n",
    "    ('scikit_learn', 'scikit-learn'),\n",
    "]\n",
    "for import_name, pip_name in REQUIRED:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        print(f\"[Deps] Installing missing package: {pip_name}\")\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', pip_name], check=False)\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# ---------------- Dataset Loading ----------------\n",
    "assert EXTENDED_PATH.exists(), f\"Missing extended dataset at {EXTENDED_PATH}\"\n",
    "assert SCHEMA_PATH.exists(),   f\"Missing schema dataset at {SCHEMA_PATH}\"\n",
    "\n",
    "with EXTENDED_PATH.open('r', encoding='utf-8') as f:\n",
    "    extended_data = json.load(f)\n",
    "with SCHEMA_PATH.open('r', encoding='utf-8') as f:\n",
    "    schema_data = json.load(f)\n",
    "print(f\"[Load] Extended: {len(extended_data)} | Schema: {len(schema_data)}\")\n",
    "\n",
    "# ---------------- Normalization ----------------\n",
    "def normalize(entry):\n",
    "    q = (entry.get('question') or '').strip()\n",
    "    sparql = (entry.get('sparql') or '').strip()\n",
    "    entities = entry.get('entities') or entry.get('entity') or []\n",
    "    if isinstance(entities, str):\n",
    "        entities_list = [e.strip() for e in entities.split('\\n') if e.strip()]\n",
    "    else:\n",
    "        entities_list = entities\n",
    "    entity_block = '\\n'.join(entities_list) if entities_list else ''\n",
    "    return {\n",
    "        'question': q,\n",
    "        'entities_list': entities_list,\n",
    "        'entity_block': entity_block,\n",
    "        'sparql': sparql\n",
    "    }\n",
    "\n",
    "normalized = [normalize(e) for e in extended_data] + [normalize(e) for e in schema_data]\n",
    "print(f\"[Normalize] Total normalized: {len(normalized)}\")\n",
    "\n",
    "inputs, targets = [], []\n",
    "for rec in normalized:\n",
    "    if rec['question'] and rec['sparql']:\n",
    "        ent_part = f\"\\nentity: {rec['entity_block']}\" if rec['entity_block'] else ''\n",
    "        inputs.append(f\"task: generate_sparql\\ninput: {rec['question']}{ent_part}\")\n",
    "        targets.append(rec['sparql'])\n",
    "print(f\"[Pairs] Valid NL->SPARQL pairs: {len(inputs)}\")\n",
    "\n",
    "# ---------------- Split ----------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    inputs, targets, test_size=TRAIN_VAL_SPLIT, random_state=SEED\n",
    ")\n",
    "print(f\"[Split] Train: {len(train_inputs)} | Val: {len(val_inputs)}\")\n",
    "\n",
    "# Save combined corpus (optional)\n",
    "with open('merged_combined_corpus.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump([{'input_text': i, 'target_text': t} for i, t in zip(inputs, targets)], f, ensure_ascii=False, indent=2)\n",
    "print('[Save] merged_combined_corpus.json written')\n",
    "\n",
    "# ---------------- Model / Tokenizer Source ----------------\n",
    "model_source = None\n",
    "for cand in PREV_CHECKPOINT_CANDIDATES:\n",
    "    if Path(cand).exists():\n",
    "        model_source = cand\n",
    "        break\n",
    "if model_source is None:\n",
    "    model_source = BASE_MODEL_NAME\n",
    "print(f\"[Model] Using source: {model_source}\")\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq, EarlyStoppingCallback, TrainerCallback\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "try:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_source)\n",
    "    print(f\"[Tokenizer] Loaded from {model_source} | vocab={len(tokenizer)}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Tokenizer] Failed from {model_source}: {e} -> fallback to {BASE_MODEL_NAME}\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# Model\n",
    "try:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_source, device_map=None)\n",
    "    print(f\"[Model] Loaded: {model_source}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Model] Failed from {model_source}: {e} -> fallback to {BASE_MODEL_NAME}\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# Verify embedding vs vocab\n",
    "embed_size = model.get_input_embeddings().weight.shape[0]\n",
    "if embed_size != len(tokenizer):\n",
    "    print(f\"[WARNING] Vocab mismatch (embed={embed_size} vs tok={len(tokenizer)}). Aborting to prevent runtime errors.\")\n",
    "    raise SystemExit(1)\n",
    "else:\n",
    "    print(f\"[Check] Vocab sizes aligned: {embed_size}\")\n",
    "\n",
    "# ---------------- Device + Memory ----------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[Device] Using: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Mem: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"[Warn] No GPU detected; training will be slow.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ---------------- Metrics Fallback System ----------------\n",
    "import evaluate\n",
    "\n",
    "def safe_load_metric(name, pip_pkg=None, alt=None):\n",
    "    if name in ['rouge', 'bleu', 'meteor'] and alt:\n",
    "        print(f\"[Metric] Direct fallback for {name}\")\n",
    "        return alt, True\n",
    "    try:\n",
    "        m = evaluate.load(name)\n",
    "        def _call(preds, refs):\n",
    "            return m.compute(predictions=preds, references=refs)\n",
    "        return _call, False\n",
    "    except Exception as e:\n",
    "        print(f\"[Metric] Failed load {name}: {e}\")\n",
    "        if pip_pkg:\n",
    "            subprocess.run([sys.executable,'-m','pip','install','--quiet',pip_pkg], check=False)\n",
    "            try:\n",
    "                m = evaluate.load(name)\n",
    "                def _call(preds, refs):\n",
    "                    return m.compute(predictions=preds, references=refs)\n",
    "                return _call, False\n",
    "            except Exception:\n",
    "                pass\n",
    "        if alt:\n",
    "            print(f\"[Metric] Using fallback for {name}\")\n",
    "            return alt, True\n",
    "        return (lambda p,r: {}), True\n",
    "\n",
    "# Fallbacks\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rouge_fallback(preds, refs):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "    r1=r2=rl=0; n=len(preds)\n",
    "    for p,r in zip(preds, refs):\n",
    "        s=scorer.score(r,p)\n",
    "        r1+=s['rouge1'].fmeasure; r2+=s['rouge2'].fmeasure; rl+=s['rougeL'].fmeasure\n",
    "    return {'rouge1': r1/n if n else 0.0,'rouge2': r2/n if n else 0.0,'rougeL': rl/n if n else 0.0}\n",
    "\n",
    "import sacrebleu\n",
    "\n",
    "def bleu_fallback(preds, refs):\n",
    "    return {'bleu': sacrebleu.corpus_bleu(preds,[refs]).score}\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def meteor_fallback(preds, refs):\n",
    "    scores=[meteor_score([r],p) for p,r in zip(preds,refs)]\n",
    "    return {'meteor': sum(scores)/len(scores) if scores else 0.0}\n",
    "\n",
    "try:\n",
    "    from bert_score import score as bert_score\n",
    "    def bertscore_fallback(preds, refs):\n",
    "        P,R,F = bert_score(preds, refs, lang='en', verbose=False)\n",
    "        return {'bertscore_precision': float(P.mean()), 'bertscore_recall': float(R.mean()), 'bertscore_f1': float(F.mean())}\n",
    "except Exception:\n",
    "    def bertscore_fallback(preds, refs):\n",
    "        return {'bertscore_f1': 0.0}\n",
    "\n",
    "metric_rouge, r_fb = safe_load_metric('rouge', 'rouge-score', rouge_fallback)\n",
    "metric_bleu,  b_fb = safe_load_metric('bleu', 'sacrebleu', bleu_fallback)\n",
    "metric_meteor,m_fb = safe_load_metric('meteor','nltk', meteor_fallback)\n",
    "metric_bertscore, bs_fb = safe_load_metric('bertscore','bert-score', bertscore_fallback)\n",
    "print('[Metrics] Fallback usage:', {'rouge':r_fb,'bleu':b_fb,'meteor':m_fb,'bertscore':bs_fb})\n",
    "\n",
    "# ---------------- Dataset Objects ----------------\n",
    "from datasets import Dataset, DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict({'input_text': train_inputs, 'target_text': train_targets}),\n",
    "    'validation': Dataset.from_dict({'input_text': val_inputs, 'target_text': val_targets}),\n",
    "})\n",
    "\n",
    "label_pad_token_id = -100\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(batch['input_text'], max_length=MAX_SOURCE_LEN, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(text_target=batch['target_text'], max_length=MAX_TARGET_LEN, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = raw_datasets.map(preprocess, batched=True, remove_columns=['input_text','target_text'])\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=label_pad_token_id)\n",
    "\n",
    "# ---------------- Metrics Callback ----------------\n",
    "from transformers import TrainerCallback\n",
    "class MetricsTableCallback(TrainerCallback):\n",
    "    def __init__(self, save_csv_path='epoch_metrics_log.csv'): self.save_csv_path=save_csv_path; self.rows=[]\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        row={'step': state.global_step}\n",
    "        for k,v in metrics.items():\n",
    "            if isinstance(v,(int,float)): row[k]=v\n",
    "        self.rows.append(row); return control\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if not self.rows: return\n",
    "        import csv\n",
    "        keys=sorted({k for r in self.rows for k in r.keys()})\n",
    "        with open(self.save_csv_path,'w',newline='',encoding='utf-8') as f:\n",
    "            w=csv.DictWriter(f, fieldnames=keys); w.writeheader(); [w.writerow(r) for r in self.rows]\n",
    "        print(f\"[Metrics] Saved log -> {self.save_csv_path}\")\n",
    "metrics_callback = MetricsTableCallback()\n",
    "\n",
    "# ---------------- Compute Metrics (wrapped safe decode) ----------------\n",
    "\n",
    "def base_compute(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple): preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels= [l.strip() for l in decoded_labels]\n",
    "    rouge_res = metric_rouge(decoded_preds, decoded_labels)\n",
    "    bleu_res  = metric_bleu(decoded_preds, decoded_labels)\n",
    "    meteor_res= metric_meteor(decoded_preds, decoded_labels)\n",
    "    bert_res  = metric_bertscore(decoded_preds, decoded_labels)\n",
    "    metrics = {}\n",
    "    for d in (rouge_res, bleu_res, meteor_res, bert_res): metrics.update({k: float(v) for k,v in d.items()})\n",
    "    return metrics\n",
    "\n",
    "# Guard against OOB ids\n",
    "\n",
    "def clamp_oob_ids(seqs: List[List[int]], vocab_limit: int) -> int:\n",
    "    pad_id = tokenizer.pad_token_id or 0\n",
    "    fixes=0\n",
    "    for s in seqs:\n",
    "        for i,tok in enumerate(s):\n",
    "            if tok < 0 or tok >= vocab_limit:\n",
    "                s[i]=pad_id; fixes+=1\n",
    "    return fixes\n",
    "\n",
    "\n",
    "def safe_compute(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple): preds = preds[0]\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        seqs = preds.tolist()\n",
    "        fix_count = clamp_oob_ids(seqs, len(tokenizer))\n",
    "        if fix_count: print(f\"[SafeDecode] Corrected {fix_count} OOB token ids\")\n",
    "        preds = torch.tensor(seqs, device=preds.device)\n",
    "    return base_compute((preds, labels))\n",
    "\n",
    "compute_metrics = safe_compute\n",
    "\n",
    "# ---------------- Training Arguments ----------------\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
    "OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(OUTPUT_BASE),\n",
    "    evaluation_strategy=EVAL_STRATEGY,\n",
    "    save_strategy=EVAL_STRATEGY,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=250,\n",
    "    per_device_train_batch_size=BATCH_PER_DEVICE,\n",
    "    per_device_eval_batch_size=BATCH_PER_DEVICE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    num_train_epochs=TARGET_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=(torch.cuda.is_available() and USE_FP16),\n",
    "    generation_max_length=GEN_MAX_LEN,\n",
    "    generation_num_beams=GEN_NUM_BEAMS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC_FOR_BEST,\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    "    dataloader_num_workers=0 if sys.platform.startswith('win') else 2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE), metrics_callback]\n",
    ")\n",
    "\n",
    "print('\\n=== TRAIN CONFIG SUMMARY ===')\n",
    "print(f\"Device: {device} | FP16: {train_args.fp16}\")\n",
    "print(f\"Train samples: {len(tokenized['train'])} | Val samples: {len(tokenized['validation'])}\")\n",
    "print(f\"Epochs: {train_args.num_train_epochs} | LR: {train_args.learning_rate}\")\n",
    "print(f\"Effective batch size: {train_args.per_device_train_batch_size * train_args.gradient_accumulation_steps}\")\n",
    "print(f\"Output dir: {train_args.output_dir}\")\n",
    "print('============================\\n')\n",
    "\n",
    "train_result = trainer.train()\n",
    "print('[Train] Finished training.')\n",
    "metrics = train_result.metrics\n",
    "\n",
    "# Save best model + tokenizer\n",
    "trainer.save_model(str(OUTPUT_BASE / 'final'))\n",
    "tokenizer.save_pretrained(str(OUTPUT_BASE / 'final'))\n",
    "print(f\"[Save] Final model saved to {OUTPUT_BASE / 'final'}\")\n",
    "\n",
    "# Save metrics JSON\n",
    "overall_metrics_path = OUTPUT_BASE / 'training_metrics.json'\n",
    "with overall_metrics_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"[Save] Training metrics -> {overall_metrics_path}\")\n",
    "\n",
    "# Quick smoke test\n",
    "try:\n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    smoke_tokenizer = T5Tokenizer.from_pretrained(str(OUTPUT_BASE / 'final'))\n",
    "    smoke_model = T5ForConditionalGeneration.from_pretrained(str(OUTPUT_BASE / 'final')).to(device)\n",
    "    sample = random.choice(list(zip(val_inputs, val_targets)))\n",
    "    print('\\n[SmokeTest] Input snippet:')\n",
    "    print(sample[0][:250])\n",
    "    enc = smoke_tokenizer(sample[0], return_tensors='pt', truncation=True, padding='max_length', max_length=MAX_SOURCE_LEN).to(device)\n",
    "    out_ids = smoke_model.generate(enc['input_ids'], max_length=256, num_beams=4)\n",
    "    pred = smoke_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    print('[SmokeTest] Target:', sample[1][:250])\n",
    "    print('[SmokeTest] Pred  :', pred[:250])\n",
    "except Exception as e:\n",
    "    print('[SmokeTest] Failed:', e)\n",
    "\n",
    "print('\\n[Done] Unified single-cell pipeline complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Cell: Diagnose out-of-range token IDs during evaluation\n",
    "# Run AFTER the unified training cell fails, without re-initializing kernel, to inspect offending IDs.\n",
    "import torch, itertools, json\n",
    "from collections import Counter\n",
    "\n",
    "print('[Debug] Starting OOB token diagnostics...')\n",
    "\n",
    "# 1. Basic tokenizer / sentencepiece stats\n",
    "try:\n",
    "    piece_size = tokenizer.sp_model.get_piece_size()\n",
    "except Exception:\n",
    "    piece_size = tokenizer.sp_model.piece_size() if hasattr(tokenizer.sp_model,'piece_size') else None\n",
    "print(f\"[Debug] SentencePiece piece_size: {piece_size}\")\n",
    "print(f\"[Debug] len(tokenizer): {len(tokenizer)}\")\n",
    "\n",
    "# 2. Inspect embedding matrix size\n",
    "try:\n",
    "    embed_rows = model.get_input_embeddings().weight.shape[0]\n",
    "    print(f\"[Debug] Embedding rows: {embed_rows}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Debug] Could not read embedding size: {e}\")\n",
    "\n",
    "# 3. Force a single eval batch forward + generate to capture raw ids\n",
    "sample_inputs = val_inputs[:4]\n",
    "enc = tokenizer(sample_inputs, return_tensors='pt', truncation=True, padding='max_length', max_length=128).to(model.device)\n",
    "with torch.no_grad():\n",
    "    gen = model.generate(enc['input_ids'], max_length=64, num_beams=1, do_sample=False)\n",
    "print(f\"[Debug] Generated shape: {gen.shape}\")\n",
    "\n",
    "# 4. Collect all ids\n",
    "all_ids = gen.view(-1).tolist()\n",
    "min_id, max_id = min(all_ids), max(all_ids)\n",
    "print(f\"[Debug] Min id: {min_id} | Max id: {max_id}\")\n",
    "\n",
    "# 5. Find offending ids\n",
    "bad_ids = sorted({tid for tid in all_ids if tid < 0 or (piece_size is not None and tid >= piece_size)})\n",
    "print(f\"[Debug] Offending ids count: {len(bad_ids)}\")\n",
    "if bad_ids:\n",
    "    print(f\"[Debug] First up to 20 offending ids: {bad_ids[:20]}\")\n",
    "    freq = Counter([tid for tid in all_ids if tid in bad_ids])\n",
    "    print('[Debug] Top offending id frequencies:', freq.most_common(10))\n",
    "else:\n",
    "    print('[Debug] No offending ids in test generation batch.')\n",
    "\n",
    "# 6. Map a few good/bad tokens back to pieces safely\n",
    "safe_sample_ids = [i for i in all_ids if 0 <= i < (piece_size or len(tokenizer))][:10]\n",
    "print('[Debug] Sample valid tokens -> pieces:')\n",
    "for i in safe_sample_ids:\n",
    "    try:\n",
    "        print(i, '->', tokenizer.sp_model.IdToPiece(i))\n",
    "    except Exception as e:\n",
    "        print(i, '-> <error>', e)\n",
    "\n",
    "print('[Debug] Diagnostics complete. If bad_ids non-empty, we will patch safe_compute next to hard clamp to piece_size-1.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Cell: Harden safe_compute with piece_size clamp and fallback manual decoding\n",
    "from typing import Iterable\n",
    "import torch\n",
    "\n",
    "print('[Patch] Installing hardened safe_compute with piece_size clamp...')\n",
    "\n",
    "piece_size = None\n",
    "try:\n",
    "    piece_size = tokenizer.sp_model.get_piece_size()\n",
    "except Exception:\n",
    "    if hasattr(tokenizer.sp_model, 'piece_size'):\n",
    "        piece_size = tokenizer.sp_model.piece_size()\n",
    "\n",
    "if piece_size is None:\n",
    "    print('[Patch] WARNING: Could not obtain piece_size; will use len(tokenizer).')\n",
    "    piece_size = len(tokenizer)\n",
    "\n",
    "pad_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "# Manual decode to avoid sentencepiece exceptions mid-loop\n",
    "\n",
    "def manual_decode(batch_ids: Iterable[Iterable[int]]):\n",
    "    texts = []\n",
    "    for seq in batch_ids:\n",
    "        pieces = []\n",
    "        for tid in seq:\n",
    "            if tid == pad_id:\n",
    "                continue\n",
    "            if 0 <= tid < piece_size:\n",
    "                try:\n",
    "                    pieces.append(tokenizer.sp_model.IdToPiece(int(tid)))\n",
    "                except Exception:\n",
    "                    # skip any anomalous id\n",
    "                    continue\n",
    "        # Basic post-processing similar to T5 decode cleanup\n",
    "        text = ''.join(pieces).replace('â–', ' ').strip()\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Re-wrap compute\n",
    "old_base_compute = base_compute if 'base_compute' in globals() else None\n",
    "\n",
    "if old_base_compute is None:\n",
    "    print('[Patch] ERROR: base_compute not found; cannot wrap. Run unified cell first.')\n",
    "else:\n",
    "    def hardened_safe_compute(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        tensor_mode = isinstance(preds, torch.Tensor)\n",
    "        if tensor_mode:\n",
    "            seqs = preds.tolist()\n",
    "        else:\n",
    "            seqs = preds\n",
    "        # Clamp\n",
    "        oob = 0\n",
    "        for s in seqs:\n",
    "            for i, tid in enumerate(s):\n",
    "                if tid < 0 or tid >= piece_size:\n",
    "                    s[i] = pad_id\n",
    "                    oob += 1\n",
    "        if oob:\n",
    "            print(f\"[Harden] Clamped {oob} OOB ids to pad_id {pad_id}\")\n",
    "        # Try normal decode; if it fails, fallback to manual\n",
    "        try:\n",
    "            if tensor_mode:\n",
    "                preds_tensor = torch.tensor(seqs, device=preds.device)\n",
    "            else:\n",
    "                preds_tensor = torch.tensor(seqs)\n",
    "            return old_base_compute((preds_tensor, labels))\n",
    "        except Exception as e:\n",
    "            print('[Harden] Standard decode failed, using manual fallback:', e)\n",
    "            # Manual decode path replicating base_compute metric usage\n",
    "            manual_texts = manual_decode(seqs)\n",
    "            # Decode labels normally with masking\n",
    "            label_lists = []\n",
    "            for label_seq in labels:\n",
    "                label_lists.append([(l if l != -100 else pad_id) for l in label_seq])\n",
    "            decoded_labels = tokenizer.batch_decode(label_lists, skip_special_tokens=True)\n",
    "            # Metrics direct\n",
    "            rouge_res = metric_rouge(manual_texts, decoded_labels)\n",
    "            bleu_res  = metric_bleu(manual_texts, decoded_labels)\n",
    "            meteor_res= metric_meteor(manual_texts, decoded_labels)\n",
    "            bert_res  = metric_bertscore(manual_texts, decoded_labels)\n",
    "            metrics = {}\n",
    "            for d in (rouge_res, bleu_res, meteor_res, bert_res):\n",
    "                metrics.update({k: float(v) for k,v in d.items()})\n",
    "            return metrics\n",
    "    compute_metrics = hardened_safe_compute\n",
    "    trainer.compute_metrics = compute_metrics\n",
    "    print('[Patch] Hardened safe_compute installed. Re-run evaluation or training resume.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
