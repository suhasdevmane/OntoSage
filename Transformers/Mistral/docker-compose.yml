version: "3.0"
services:
 ollama:
    container_name: ollama
    build:
      context: .
    pull_policy: always
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_MODELS=/usr/share/ollama/.ollama/models
      # Space or comma separated list of models to auto-pull & optionally warm up
      - AUTO_PULL_MODELS=mistral:latest
      # Set to 'false' to skip warmup token generation after pull
      - WARMUP_MODELS=true
    healthcheck:
      test: "ollama --version && ollama ps || exit 1"
      interval: 30s
      timeout: 180s
      retries: 3
      start_period: 30s
    # deploy:
    #   resources:
    #     reservations:
          # devices:
          #   - driver: nvidia
          #     count: 1
          #     capabilities: [gpu]
    volumes:
      - .:/usr/share/ollama/.ollama/models

volumes:
  ollama-models: