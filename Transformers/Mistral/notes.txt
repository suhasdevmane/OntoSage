docker build --pull -t ollama .

docker run -d --name suhas_summary_using_ollama --restart unless-stopped -p 11434:11434 -e OLLAMA_NUM_PARALLEL=4 -e OLLAMA_MAX_LOADED_MODELS=1 -e OLLAMA_MODELS=/usr/share/ollama/.ollama/models ollama


Step 1: Verify the Docker Container is Running
docker ps

CONTAINER ID   IMAGE   COMMAND          CREATED        STATUS        PORTS                   NAMES
abc123def456   ollama  "/entrypoint.sh"  5 minutes ago  Up 5 minutes  0.0.0.0:11434->11434/tcp suhas_summary_using_ollama
2. Restart if Necessary: If the container isn’t running, restart it
docker restart suhas_summary_using_ollama

Step 2: Confirm Ollama Server is Active

docker logs suhas_summary_using_ollama

docker restart suhas_summary_using_ollama

docker exec -it suhas_summary_using_ollama bash

Check Ollama Process: Inside the container, run

ps aux | grep ollama

List Loaded Models: Still inside the container, run
ollama list

If mistral:latest isn’t listed, it didn’t pull successfully. Run
ollama pull mistral:latest

curl http://localhost:11434


Exit Container
exit

Step 4: Test External Accessibility

Test from Your Local Machine: 

Invoke-WebRequest -Uri "http://<public-ip-or-dns>:11434"
Invoke-WebRequest -Uri "http://10.98.45.43:11434"

test in python
import requests
url = "http://<public-ip-or-dns>:11434"
response = requests.get(url)
print("Status Code:", response.status_code)
print("Response:", response.text)


from ollama import Client

# Replace with your server's public IP/DNS
client = Client(host="http://<public-ip-or-dns>:11434")

response = client.generate(
    model="mistral:latest",
    prompt="Test the Mistral model",
    options={"max_tokens": 50}
)
print("Mistral Response:", response["response"])


NEW=====


---

### 1. Create the Docker Network
Run this command on your server to create a user-defined network:

```bash
docker network create summary_network_suhas
```

- This creates a bridge network named `summary_network_suhas` that allows containers to communicate using their container names as hostnames.

---

### 2. Updated Docker Command for Ollama
Here’s your updated command to run the Ollama container with the Mistral model on the new network. It’s based on your provided Dockerfile and `entrypoint.sh`:

```bash
docker run -d --name suhas_summary_using_ollama --restart unless-stopped --network summary_network_suhas -p 11434:11434 -e OLLAMA_NUM_PARALLEL=4 -e OLLAMA_MAX_LOADED_MODELS=1 -e OLLAMA_MODELS=/usr/share/ollama/.ollama/models ollama 

#### Changes:
- Added `--network summary_network_suhas`: Connects the container to the custom network.
- Kept everything else the same: port mapping (`-p 11434:11434`), environment variables, and container name.

#### Prerequisites:
- Build the Docker image first if you haven’t:
  ```bash
  docker build --pull -t ollama .
  ```
- Ensure your server has enough RAM (at least 5 GB free) to load `mistral:latest`, or you’ll hit the memory error again.

---

### 3. Updated ngrok Command
Here’s the updated ngrok command to expose the Ollama service on the same network:

```bash
docker run -d --name ngrok-suhas_summary_using_ollama --network summary_network_suhas -e NGROK_AUTHTOKEN=2XzP2J1VpPiGZlzfOf2igFdIjhh_hg4kYbuU19h1UaoaSp8v -p 4044:4040 ngrok/ngrok:latest http suhas_summary_using_ollama:11434 --domain=dashing-sunfish-curiously.ngrok-free.app
```

#### Changes:
- Removed `-it`: Unnecessary for a detached background process.
- Removed `-e NGROK_REGION=Global`: `Global` isn’t a valid region; ngrok defaults to `us` if unspecified, which is fine unless you need a specific region (e.g., `eu`).
- Added `--network summary_network_suhas`: Connects ngrok to the same network as the Ollama container.
- Kept `http suhas_summary_using_ollama:11434`: Since both containers are on the same network, ngrok can resolve `suhas_summary_using_ollama` as the hostname and forward traffic to port 11434.

#### Prerequisites:
- Ensure your ngrok token supports reserved domains and the domain `dashing-sunfish-curiously.ngrok-free.app` is configured in your ngrok dashboard (requires a paid plan).

---

### Full Workflow
1. **Create the Network**:
   ```bash
   docker network create summary_network_suhas
   ```

2. **Build the Ollama Image** (if not already built):
   ```bash
   docker build --pull -t ollama .
   ```

3. **Run the Ollama Container**:
   ```bash
   docker run -d --name suhas_summary_using_ollama \
     --restart unless-stopped \
     --network summary_network_suhas \
     -p 11434:11434 \
     -e OLLAMA_NUM_PARALLEL=4 \
     -e OLLAMA_MAX_LOADED_MODELS=1 \
     -e OLLAMA_MODELS=/usr/share/ollama/.ollama/models \
     ollama
   ```

4. **Run the ngrok Container**:
   ```bash
   docker run -d --name ngrok-suhas_summary_using_ollama \
     --network summary_network_suhas \
     -e NGROK_AUTHTOKEN=2XzP2J1VpPiGZlzfOf2igFdIjhh_hg4kYbuU19h1UaoaSp8v \
     -p 4044:4040 \
     ngrok/ngrok:latest http suhas_summary_using_ollama:11434 \
     --domain=dashing-sunfish-curiously.ngrok-free.app
   ```

---

### Verification
1. **Check Containers**:
   ```bash
   docker ps
   ```
   Ensure both `suhas_summary_using_ollama` and `ngrok-suhas_summary_using_ollama` are running.

2. **Check Ollama Logs**:
   ```bash
   docker logs suhas_summary_using_ollama
   ```
   Look for “Starting Ollama server” and confirmation that `mistral:latest` was pulled.

3. **Check ngrok Logs**:
   ```bash
   docker logs ngrok-suhas_summary_using_ollama
   ```
   Look for a forwarding line like:
   ```
   Forwarding    https://dashing-sunfish-curiously.ngrok-free.app -> http://suhas_summary_using_ollama:11434
   ```

4. **Test the ngrok URL**:
   On your local machine (Windows):
   ```powershell
   Invoke-WebRequest -Uri "http://dashing-sunfish-curiously.ngrok-free.app"
   ```
   Or in Python:
   ```python
   import requests
   response = requests.get("http://dashing-sunfish-curiously.ngrok-free.app")
   print(response.status_code, response.text)
   ```
   Expected: `200`, `Ollama is running`.

5. **Test Mistral Model**:
   ```python
   from ollama import Client
   client = Client(host="http://dashing-sunfish-curiously.ngrok-free.app")
   response = client.generate(
       model="mistral:latest",
       prompt="Test the Mistral model",
       options={"max_tokens": 50}
   )
   print("Mistral Response:", response["response"])
   ```

---

### Troubleshooting
- **Memory Error**: If you see the earlier memory error, upgrade your server (e.g., to `t3.medium` on AWS).
- **ngrok Connection Failure**: If logs show “cannot connect,” verify the Ollama container is running and the network is set up correctly.
- **Domain Issues**: Ensure your ngrok token and domain are valid.

Let me know the results of running these commands or any errors you encounter!