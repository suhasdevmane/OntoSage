# Rasa Custom Actions (Building 1)

This directory contains the highâ€‘leverage orchestration logic that turns a user question into:

Natural Language â†’ (Optional) NLâ†’SPARQL Translation â†’ SPARQL (Fuseki) â†’ Ontology Results â†’ (Optional) Timeseries UUID Extraction â†’ (Optional) Analytics Microservice â†’ Local LLM Summarization (Ollama/Mistral) â†’ Final Chat Response + Artifacts.

---
## ğŸš© Recent Enhancements (Oct 2025)

| Area | Change | Rationale |
|------|--------|-----------|
| Sensor Mapping | Unified cached loader `_load_sensor_uuid_map()` with env override `SENSOR_UUIDS_FILE` | Eliminates duplicate loaders & silent file creation; predictable resolution order |
| Analytics Type Selection | Decider + heuristic fallback + validation against dynamic registry | Robust, extensible, avoids stale/unsupported analytics types |
| Message Verbosity | Frontend â€œDetailsâ€ toggle + `emit_message(detail=True)` gating | Cleaner UX for nonâ€‘technical users while retaining deep logs on demand |
| Summarization | Minimal ontologyâ€‘only prompt when no timeseries UUIDs; enriched path only for analytics/timeseries flows | Reduces token noise & latency, improves relevance |
| SPARQL Reliability | Case normalization retry for `_sensor_` â†’ `_Sensor_` + full prefixed query logging | Prevents silent false negatives due to naming inconsistencies |
| Error Visibility | Autoâ€‘critical bypass for messages containing error/failure/timeout keywords | Ensures important failures still surface when details are hidden |
| Artifacts | Consistent perâ€‘user artifacts directory with timestamped JSON dumps | Traceability & reproducibility of answers |

---
## ğŸ”Œ Core Responsibilities

1. Interpret intent & determine query type (listing vs metric vs unknown).
2. Optionally call NL2SPARQL translator (T5) to get a raw SPARQL skeleton.
3. Execute SPARQL against Fuseki, log raw + prefixed query, standardize bindings.
4. Extract timeseries UUIDs â†’ decide whether to branch into analytics.
5. Decide analytics type (external Decider service OR fallback heuristics).
6. Fetch SQL telemetry (MySQL for Building 1) within the requested or inferred date window.
7. Build canonical analytics payload (collapsing or preserving sensor names depending on analysis type).
8. Call analytics microservice (optional) and merge/transform response.
9. Replace UUIDs with descriptive sensor names using cached mapping.
10. Summarize (ontology-only OR analytics-enhanced) via local Mistral model.
11. Emit gated progress + always-show critical outputs to frontend; save artifacts.

---
## ğŸ§  Analytics Type Decision Flow

Decision only proceeds if at least one valid timeseries UUID is extracted. Then:

1. External Decider (`DECIDER_URL`) â†’ expects `{ perform_analytics: bool, analytics: <type> }`.
2. If absent/failure: fallback `_pick_type_from_context(question, sensor_types)` uses keyword groups:
	 - humidity â†’ `analyze_humidity`
	 - temp/temperature â†’ `analyze_temperatures`
	 - co2 â†’ `analyze_co2_levels`
	 - pm/particulate â†’ `analyze_pm_levels`
	 - correlate/correlation/relationship â†’ `correlate_sensors`
	 - anomaly/outlier/abnormal/fault/failure â†’ `detect_potential_failures`
	 - trend/time series/history/timeline/over time â†’ `analyze_sensor_trend`
	 - default fallback â†’ `analyze_sensor_trend`
3. â€œStructural / ontologyâ€ questions (keywords: label, type, class, category, installed, location, where is, which sensors, list sensors, show sensors) explicitly suppress analytics.
4. Candidate is validated against dynamic `_supported_types()` (remote registry + static fallback). Unsupported â†’ fallback heuristic.
5. Final choice stored in slot `analytics_type`; passed to `ActionProcessTimeseries` which re-validates.

Special handling:
- `correlate_sensors` keeps full sensor instance names (no collapsing) to preserve distinct series.
- `analyze_humidity` retains specific instance keys (avoids merging rooms/zones and double counting).
- All other analytics collapse multiple instances to a base sensor key (e.g., `Zone_Air_Humidity_Sensor`).

---
## ğŸ—‚ Sensor UUID Mapping Loader

Implemented once at module level:
Resolution order â†’ `SENSOR_UUIDS_FILE` (env path) â†’ `./sensor_uuids.txt` â†’ `./actions/sensor_uuids.txt`.

Features:
- Bidirectional dict (nameâ†’uuid and uuidâ†’name).
- mtime + periodic reload (`SENSOR_UUIDS_RELOAD_SEC`, default 300s).
- No silent file creation; missing file logs warning (cached data reused if available).
- Logs: path, count, malformed lines, duplicate conflicts.

Environment overrides:
```
SENSOR_UUIDS_FILE=/app/shared_data/sensor_uuids.txt
SENSOR_UUIDS_RELOAD_SEC=120
```

Usage inside actions: `self.load_sensor_mappings()` delegates to the unified loader.

---
## ğŸ“¨ Verbosity & Message Gating

Frontend sends a metadata flag (Details ON/OFF). The helper `emit_message(dispatcher, tracker, text=..., detail=True)` only sends when details are enabled unless the text is auto-classified critical (contains tokens like `error`, `failed`, `timeout`). This reduces UI noise while retaining actionable failures.

Rules:
- Empty text + no attachments are suppressed to avoid blank bubbles.
- Critical keywords bypass gating.
- Attachments (artifacts) are detail-gated announcements.

---
## ğŸ§¾ Summarization Modes

| Mode | Trigger | Prompt Contents | Exclusions |
|------|---------|-----------------|------------|
| Ontology-only | No timeseries UUIDs | Instructions + Question + Standardized JSON + note about no timeseries | Raw SPARQL, compact result list removed |
| Analytics-enriched | Timeseries path (post analytics or SQL fallback) | Instructions + Original Question + Analytics/merged JSON | N/A (can be extended later) |

LLM: Local Ollama (`mistral:latest`). Options tuned for concise summaries (max_tokens ~150â€“180). Prompt preview length & total chars logged for observability.

---
## ğŸ§ª Standard Analytics Payload Shapes

1. Nested (default for most analytics):
```jsonc
{
	"analysis_type": "analyze_temperatures",
	"1": {
		"Zone_Air_Temp_Sensor": {
			"timeseries_data": [ { "datetime": "2025-02-10 05:31:59", "reading_value": 21.4 }, ... ]
		}
	}
}
```
2. Flat (correlation):
```jsonc
{
	"analysis_type": "correlate_sensors",
	"Zone_Air_Temp_Sensor_5.01": [ { "datetime": "2025-02-10 05:31:59", "reading_value": 21.4 } ],
	"Zone_Air_Temp_Sensor_5.02": [ ... ]
}
```

---
## ğŸ—„ MySQL (Building 1 Telemetry)

Env-driven config with optional local override:

| Variable | Purpose | Default (container) |
|----------|---------|---------------------|
| USE_LOCAL_MYSQL | Switch host vs service DNS | false |
| DB_HOST / DB_PORT | MySQL service location | mysqlserver / 3306 |
| DB_USER / DB_PASSWORD | Credentials | root / mysql |
| DB_NAME | Database name | sensordb |
| DB_TABLE | Table queried for timeseries | sensor_data |
| LOCAL_DB_* | Alternative host credentials | host.docker.internal / 3306 / root / root |

Dynamic SQL selects only requested UUID columns plus `Datetime`. Single UUID queries add an `IS NOT NULL` predicate for efficiency.

---
## ğŸ” Environment Variables (Selected)

| Variable | Category | Effect |
|----------|----------|--------|
| NL2SPARQL_URL | Translation | Enables NL â†’ SPARQL; absent = direct SPARQL skip |
| DECIDER_URL | Analytics decision | External decision service for perform_analytics/type |
| ANALYTICS_URL | Analytics execution | When set, microservice call performed; else local summarization over SQL only |
| BASE_URL | Artifact hosting | Used to build download URLs in chat responses |
| SENSOR_UUIDS_FILE | Sensor mapping | Explicit mapping file path override |
| SENSOR_UUIDS_RELOAD_SEC | Sensor mapping | Cache reload window seconds |
| ANALYTICS_REGISTRY_URL | Dynamic analytics types | Remote registry union with static fallback |

---
## ğŸ§· Artifacts & File Handling

Per-user folder: `shared_data/artifacts/<sanitized_sender_id>/`

Artifacts saved:
- SPARQL standardized JSON (`sparql_response_<timestamp>.json`)
- SQL raw results (`sql_results_<epoch>.json`)
- Analytics nested payload (`analytics_payload_<epoch>.json`)

Each saved file triggers a gated attachment message with a direct link (BASE_URL + path).

---
## ğŸ›  Adding a New Analytical Skill (Extended)

1. Implement microservice handler & expose in registry (or static fallback set).
2. Update analytics service image & rebuild.
3. Extend heuristic keywords if needed (both `_pick_type_from_context` variants) until refactored to a shared util.
4. (Optional) Add domain slot mappings/intents for explicit user selection.
5. Rebuild action server; verify `_supported_types()` log includes new type.

---
## ğŸ©º Debugging Checklist

| Symptom | Check |
|---------|-------|
| No analytics executed | Were UUIDs extracted? Logs: Timeseries detection. Decider suppression? Structural keywords? |
| Empty summary | Inspect LLM prompt preview log; ensure analytics JSON not empty; confirm `mistral:latest` pulled |
| Sensors not recognized | Verify sensor mapping file path via startup logs; ensure naming case matches Brick TTL |
| â€œCase normalization retryâ€ logged | Original SPARQL returned zero results with mixed `_sensor_` casing; normalization path executed |
| Attachments missing | BASE_URL set? File save errors in logs? Verbosity toggle off (user hid Details)? |

---
## ğŸ§ª Testing & Validation

Minimal smoke workflow after changes:
1. Ask â€œList CO2 sensorsâ€. Expect ontology-only summary (no analytics).
2. Ask â€œCO2 trend today in Room 5â€. Expect analytics path + timeseries extraction + summary.
3. Toggle Details OFF â†’ intermediate progress messages suppressed; final summaries visible.
4. Rename a sensor in query with lower-case `_sensor_` â†’ verify retry logs & results appear.

---
## ğŸ”„ Future Refactors (Planned)
- Consolidate duplicated `_pick_type_from_context` into shared helper.
- Optional plugin registry for summarization strategies.
- Add caching/ETag for artifacts to reduce frontend fetch bandwidth.
- Add test harness for SPARQLâ†’standardization transformations.

---
## ğŸ§© Directory Notes
- `actions.py` â€“ Core orchestration & summarization.
- `requirements.txt` â€“ Python deps for action server environment.
- (Artifacts) `/app/shared_data/artifacts` â€“ runtime generated outputs.

---
## ğŸ“œ Legacy Notes
Prior versions created empty `sensor_uuids.txt` files when absent; this is removed to avoid masking deployment issues.

---
## âœ… Quick Reference
```
Slot path (timeseries)  : Question â†’ SPARQL â†’ UUIDs â†’ analytics_type â†’ SQL â†’ analytics (opt) â†’ summary
Slot path (ontology)    : Question â†’ SPARQL â†’ (no UUIDs) â†’ summary (minimal prompt)
Verbosity gating        : emit_message(detail=True) hidden unless user enabled Details
Sensor mapping override : SENSOR_UUIDS_FILE=/app/sensor_uuids_custom.txt
```

---
## ğŸ“ Changelog (local to actions)
- 2025-10-07: Unified sensor UUID loader; removed silent file creation; expanded README.
- 2025-10-05: Added minimal prompt mode for ontology-only summarization.
- 2025-10-04: Case normalization retry for `_sensor_` â†’ `_Sensor_` in SPARQL queries.
- 2025-10-03: Verbosity toggle + gated progress messaging.
- 2025-10-02: Dynamic analytics registry with caching.
- 2025-10-01: Initial analytics summarization refactor with Ollama prompt logging.

---
## ğŸ“ See Also
- Root project overview: `../../README.md`
- Analytics details: `../../analytics.md`
- Buildings taxonomy: `../../BUILDINGS.md`

---
## ğŸ”„ Full Action Server Lifecycle

High-level event chain when a user sends a message through the REST/Socket channel:

1. Rasa Core receives the user message â†’ intent + entities are parsed.
2. Policies predict `action_question_to_brickbot` (for analytical / ontology queries) OR forms if slot collection needed.
3. `ActionQuestionToBrickbot.run()` executes a staged pipeline (instrumented by `PipelineLogger`):
	1. extract_user_message
	2. nl2sparql_translate (optional) â†’ obtains raw SPARQL
	3. fuseki_query â†’ executes prefixed SPARQL
	4. format_results â†’ human-readable short form for debug
	5. standardize â†’ produce normalized JSON structure
	6. summarize_without_timeseries OR branch to date / analytics selection
4. If timeseries UUIDs found â†’ slots set (`timeseries_ids`, `analytics_type`) â†’ followâ€‘up triggers `action_process_timeseries`.
5. `ActionProcessTimeseries.run()` stages:
	1. collect_slots â†’ read required IDs/dates
	2. normalize_dates â†’ accept many user formats
	3. mysql_fetch â†’ dynamic SELECT by UUID columns
	4. analytics_call (optional) â†’ microservice POST
	5. uuid_replace â†’ user friendly sensor names
	6. summarize_timeseries â†’ LLM summary
6. Final messages + artifacts are emitted back to Rasa â†’ returned to channel (frontend) as an ordered list of bot messages.

---
## ğŸ§µ Sequence (Ontology + Analytics Branch)

```
User â†’ Rasa â†’ action_question_to_brickbot
  â”œâ”€ (Intent/slots) â†’ Heuristic query type
  â”œâ”€ (Optional) NL2SPARQL â†’ raw SPARQL
  â”œâ”€ Prefix augmentation â†’ full SPARQL
  â”œâ”€ Fuseki â†’ JSON bindings
  â”œâ”€ Standardize â†’ uniform results list
  â”œâ”€ Extract UUIDs?
  â”‚    â”œâ”€ No â†’ Ontology-only summarize â†’ reply
  â”‚    â””â”€ Yes â†’ Decide analytics (Decider / heuristic)
  â”‚          â”œâ”€ Perform? = false â†’ Ontology + minimal timeseries mention â†’ reply (or ask dates)
  â”‚          â””â”€ Perform? = true â†’ Set slots â†’ FollowupAction(action_process_timeseries)
  â””â”€ (If Followup) â†’ action_process_timeseries
			â”œâ”€ Date normalization
			â”œâ”€ MySQL fetch
			â”œâ”€ Build canonical payload
			â”œâ”€ (Optional) analytics microservice
			â”œâ”€ UUIDâ†’Name replacement
			â”œâ”€ LLM summarization
			â””â”€ Reply + artifacts
```

---
## ğŸ“‚ Data & Document Sharing Model

| Data Type | Origin | Persistence | Exposure Path |
|-----------|--------|-------------|---------------|
| SPARQL standardized JSON | Fuseki query result | `shared_data/artifacts/<user>/sparql_response_<ts>.json` | HTTP file server (BASE_URL/artifacts/...) |
| SQL raw results | MySQL dynamic SELECT | `sql_results_<epoch>.json` | HTTP file server (link gated under Details) |
| Analytics payload (nested/flat) | Aggregated SQL (and possibly analytics microservice request body) | `analytics_payload_<epoch>.json` | HTTP file server (debug) |
| Analytics response (optional) | Microservice POST /analytics/run | In-memory only (truncated logs); attach if needed later | (Future: writable artifact) |
| Summaries | LLM output | Ephemeral (log lines only) | Chat message text |
| Mapping file | Host bind / env path | Not copied; cached in-process | Not exposed (internal only) |

Artifacts are strictly immutable once written (timestamped). Frontend can download them directly or display inline (JSON viewer) if implemented.

---
## ğŸ§± Caching Layers

| Layer | Mechanism | Invalidation | Notes |
|-------|-----------|--------------|-------|
| Sensor UUID map | In-memory dict with mtime & age check | File mtime change OR > reload window | Avoids repeated disk IO |
| Analytics registry | Remote fetch + TTL (not shown here but referenced via `_supported_types()`) | Time-based | Fallback static set ensures resilience |
| SPARQL results | None (fresh each query) | N/A | Could add per-question cache if needed |
| SQL results | None | N/A | Rely on DB indexes + narrow SELECT |
| LLM prompt/summary | None | N/A | Deterministic caching possible for identical structured inputs |

Potential future optimization: add a digest cache keyed by (question, sensor_types, date_window) â†’ reuse analytics results when identical.

---
## ğŸš¨ Error & Resilience Strategy

| Failure Point | Handling | User Feedback | Escalation |
|---------------|----------|---------------|------------|
| NL2SPARQL timeout / error | Sets translation_error; may prompt for sensor or abort | Template: `utter_translation_error` | Log correlation ID for trace |
| SPARQL execution error | Abort early, set `sparql_error` slot | â€œError executing SPARQL queryâ€ | Retry attempt only for case normalization scenario |
| Empty SPARQL + metric intent | Prompt for `sensor_type` form | â€œI need to know which sensor type...â€ | None |
| No UUIDs found | Summarize ontology-only | Summary of entities/relationships | Suggest specifying sensor if user wants metrics |
| Decider unavailable | Fallback heuristics | None (transparent) | Log warning only |
| Analytics microservice error | Log + fallback to SQL-only summary | â€œAnalytics error: â€¦â€ or generic error | Consider marking in summary prefix |
| MySQL fetch error | Returns explicit message | â€œMySQL error: â€¦â€ | None (user can retry) |
| LLM (Ollama) failure | Returns no summary (safe) | â€œUnable to generate summary.â€ | Log stacktrace |
| Missing mapping file | Warning + use UUIDs raw | No direct user error; names appear as UUIDs | Encourage deployment fix via logs |

Critical words (error/failure/timeout) bypass verbosity gating to avoid hiding actionable diagnostics.

---
## ğŸ§© Extension Points

| Goal | Where to Hook | Minimal Steps |
|------|---------------|---------------|
| Add new analytics type | Microservice + analytics registry + heuristic keywords | Implement endpoint â†’ add keyword(s) â†’ rebuild actions & microservices |
| Add new summarization mode | `summarize_response` (both classes) | Branch on marker key (e.g., `_correlation_summary`) before prompt assembly |
| Add alternate DB backend | New fetch method in `ActionProcessTimeseries` | Choose by env var (e.g., `DB_BACKEND`) then branch (MySQL / Timescale / Postgres) |
| Add caching for analytics | Wrapper around analytics call storing by (analytics_type, sensor_set, date_window) | Compute key digest; skip call if fresh |
| Export artifacts elsewhere | Post-save hook after writing JSON | Stream to object storage (S3/minio) or message queue |
| Structured telemetry diffs | Pre-summarization transformation | Create derived stats (rate of change, peak windows) before prompt |

---
## ğŸ§ª Local Development Tips

1. Run only the Action Server service with dependencies (Fuseki + MySQL + Analytics) to speed iterative cycles.
2. Use `docker compose logs -f action_server` while issuing REST requests via curl/PowerShell to observe stage timings.
3. Temporarily set `SENSOR_UUIDS_RELOAD_SEC=5` when refining mapping files.
4. Use a fixed question & slot injection in a test script to profile summarization latency.
5. Add `DETAILS=off` metadata in frontend â†’ confirm gating hides progress noise.

---
## ğŸ” Observability & Logging Conventions

| Prefix / Pattern | Meaning |
|------------------|---------|
| `[QuestionToBrickbot][<corr>] START stage 'nl2sparql'` | Stage timing envelope start |
| `Ollama summarize invocation` | Summarization request meta (chars, mode) |
| `Loaded X sensor mappings from ...` | Cache refresh success |
| `Case-normalized SPARQL retry` | Retried due to `_sensor_` case issues |
| `Standardized JSON sample:` | Truncated preview for debugging prompt inputs |

Correlate multi-stage logs via the correlation ID present in each stage line.

---
## ğŸ§ª Example End-to-End (Concrete)

User: â€œCorrelate humidity and temperature for last week in Lab 5â€
1. Intent classified as metric (keywords: correlate, humidity, temperature)
2. NL2SPARQL returns query referencing candidate sensors (may be empty entity list initially)
3. SPARQL executes; returns bindings with two UUIDs
4. Timeseries IDs extracted â†’ has_timeseries True
5. Decider (if present) returns perform_analytics true & maybe suggested type; heuristics would map correlate â†’ `correlate_sensors`
6. Slots set; FollowupAction triggers `action_process_timeseries`
7. Date phrases â€œlast weekâ€ normalized to previous ISO week bounds
8. MySQL query selects `Datetime`, UUID1, UUID2
9. Build flat correlation payload: `{ analysis_type: correlate_sensors, <Name1>: [...], <Name2>: [...] }`
10. (Optional) Analytics microservice returns correlation coefficient & any lag stats (future enhancement)
11. UUIDs replaced with friendly names
12. Summarization prompt built (analytics-enriched mode)
13. Summary returned: â€œHumidity and temperature in Lab 5 moved together (râ‰ˆ0.78) with no significant anomalies â€¦â€
14. Artifacts (SQL + payload) available via file server links for audit.

---
## ğŸ§¹ Housekeeping / Maintenance Checklist

| Frequency | Task |
|-----------|------|
| Weekly | Rotate / prune stale artifact JSONs (script forthcoming) |
| Weekly | Validate sensor UUID mapping freshness (diff against registry) |
| Monthly | Refresh analytics registry & retire unused analysis types |
| Monthly | Re-run NL2SPARQL evaluation set for drift detection |
| Quarterly | Review summarization prompt tokens & adjust max_tokens if needed |

---
## ğŸ§­ Design Trade-offs

| Decision | Trade-off | Future Option |
|----------|-----------|---------------|
| Local LLM (Ollama) | Faster, no network cost; model limited to whatâ€™s locally pulled | Remote hosted model for improved reasoning |
| On-demand analytics microservice | Flexible modular pipeline; network hop overhead | Inline light analytics for trivial stats |
| Gated verbose messages | Clean UI; may hide some contextual breadcrumbs | Add â€œshow last pipelineâ€ button |
| Per-user artifact folders | Isolation & audit; more files over time | Zip rotation / archival job |

---
## ğŸ” Security Considerations

- No user-supplied SPARQL is executed directly; queries pass through translator or curated patterns.
- File server serves static JSON only; no execution risk (enforce correct MIME types).
- Avoid leaking raw SPARQL in ontology-only summarization to reduce accidental prompt injection surface.
- Mapping file path controlled via env to prevent directory traversal injection.

---
<!-- End of actions README -->
